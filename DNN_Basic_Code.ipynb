{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyNugW7+T+h7w3Fsa5VetHk8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/growingpenguin/growingpenguin.github.io/blob/master/DNN_Basic_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation <br/>\n",
        "box plot으로  그리기 <br/>\n",
        "Target : 3295행~ 3300행(6개 데이터) <br/>\n"
      ],
      "metadata": {
        "id": "IuzoucDgkf4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting It All together"
      ],
      "metadata": {
        "id": "BCbf4ix4Pbgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare and Load Data"
      ],
      "metadata": {
        "id": "roS8a9hOPgBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "#Prepare Data\n",
        "\n",
        "#df = pd.read_csv('/content/drive/MyDrive/RsProjData/database_20231101.csv', low_memory=False)\n",
        "#df2 = df.iloc[:, 3:17]\n",
        "#df2[\"gamma\"] = df[\"gamma\"]\n",
        "\n",
        "def scale_and_encode_columns(df, scaling_columns, label_encoding_columns):\n",
        "    scaled_df = df.copy()\n",
        "\n",
        "    # Min-Max Scaling\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    for col in scaling_columns:\n",
        "        col_data = df[col].to_numpy().reshape(-1, 1)\n",
        "        scaled_data = scaler.fit_transform(col_data)\n",
        "        scaled_df[f'{col}_scaling'] = scaled_data\n",
        "\n",
        "    # Label Encoding\n",
        "    label_encoder = LabelEncoder()\n",
        "\n",
        "    for col in label_encoding_columns:\n",
        "        col_data = df[col].to_numpy().reshape(-1, 1)\n",
        "        col_data = col_data.ravel()\n",
        "        encoded_data = label_encoder.fit_transform(col_data).astype(float)\n",
        "        encoded_data = encoded_data.reshape(-1, 1)\n",
        "        scaled_df[f'{col}_scaling'] = encoded_data\n",
        "\n",
        "    return scaled_df\n",
        "\n",
        "\n",
        "# Define the columns for Min-Max Scaling\n",
        "scaling_columns = ['Model length [m]', 'Height', 'Breadth', 'Upper chamfer height', 'Upper chamfer breadth',\n",
        "                   'Lower chamfer height', 'Lower chamfer breadth', 'Volume [m3]', 'Number of sensors',\n",
        "                   'Loading [H]', 'Heading [deg]', 'Tz [s]', 'Hs [m]']\n",
        "\n",
        "# Define the columns for Label Encoding\n",
        "label_encoding_columns = ['Density Ratio']\n",
        "\n",
        "# Apply the scaling and encoding function\n",
        "df_scaled = scale_and_encode_columns(df2, scaling_columns, label_encoding_columns)\n",
        "\n",
        "# Drop unscaled columns\n",
        "columns_to_drop = [i for i in range(14)]\n",
        "df_scaled = df_scaled.drop(df_scaled.columns[columns_to_drop], axis=1)\n",
        "df_scaled = df_scaled[[col for col in df_scaled.columns if col != 'gamma'] + ['gamma']]\n",
        "\n",
        "# Create a custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.x_data = df.iloc[:, :-1].values\n",
        "        self.y_data = df.iloc[:, -1].values  # No need to convert to a DataFrame\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.FloatTensor(self.x_data[idx])\n",
        "        y = torch.FloatTensor([self.y_data[idx]])\n",
        "        return x, y\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "# Test Dataset\n",
        "test_dataset = df_scaled.iloc[3293: 3299, :]\n",
        "#Train Dataset\n",
        "indices_to_drop = [i for i in range(3293, 3299)]\n",
        "train_dataset = df_scaled.drop(indices_to_drop, axis=0)\n",
        "\n",
        "#Train/Test Dataset\n",
        "train_split = int(0.8 * len(train_dataset))\n",
        "train_ds = CustomDataset(train_dataset[:train_split])\n",
        "test_ds = CustomDataset(train_dataset[train_split:])\n",
        "val_ds = CustomDataset(test_dataset)\n",
        "\n",
        "# Create DataLoader for training, validation, and test sets\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "cols = train_dataset.columns.tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvwUaeuBPkfD",
        "outputId": "253d5ca5-cd83-4699-d11a-c815b42feb26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_ds.x_data.shape)\n",
        "print(train_ds.y_data.shape)\n",
        "print(test_ds.x_data.shape)\n",
        "print(test_ds.y_data.shape)\n",
        "print(val_ds.x_data.shape)\n",
        "print(val_ds.y_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itBnkt1tiLiO",
        "outputId": "e7cd967d-9d1a-4552-af15-58274bf8e8c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4490, 14)\n",
            "(4490,)\n",
            "(1123, 14)\n",
            "(1123,)\n",
            "(6, 14)\n",
            "(6,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Data"
      ],
      "metadata": {
        "id": "6StcDmvJXG-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_data(train_ds=train_ds,\n",
        "              test_ds=test_ds,\n",
        "              val_ds=val_ds,\n",
        "              cols = cols,\n",
        "              predictions=None):\n",
        "    \"\"\"\n",
        "    Plots training data, test data and compares predictions.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set the number of columns and rows for the subplots\n",
        "    num_columns = 2\n",
        "    num_rows = 7\n",
        "    X_train, y_train = train_ds.x_data, train_ds.y_data\n",
        "    X_test, y_test = test_ds.x_data, test_ds.y_data\n",
        "    X_val, y_val = val_ds.x_data, val_ds.y_data\n",
        "\n",
        "    # Create subplots with the specified layout\n",
        "    fig, axs = plt.subplots(num_rows, num_columns, figsize=(15, 21))\n",
        "\n",
        "    # Flatten the axs array to simplify indexing\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    for i in range(len(cols[:-1])):\n",
        "        column_name = cols[i]\n",
        "        # Index to the appropriate subplot\n",
        "        ax = axs[i]\n",
        "\n",
        "        # Plot training data in blue\n",
        "        ax.scatter(X_train[:,i], y_train, c=\"b\", s=4, label=f\"{column_name} Training data\")\n",
        "        # Plot test data in green\n",
        "        ax.scatter(X_test[:,i], y_test, c=\"g\", s=4, label=f\"{column_name} Testing data\")\n",
        "        # Plot validation data in yellow\n",
        "        ax.scatter(X_val[:,i], y_val, c=\"r\", s=4, label=f\"{column_name} Validation data\")\n",
        "\n",
        "        # Customize subplot title if needed\n",
        "        ax.set_title(f\"{column_name} data\")\n",
        "\n",
        "    # Adjust layout to prevent overlapping titles\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show the plots\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "e9tO4_KWo6R7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "26b0d40c-aa0d-4ca3-87ab-8ee1a0eff917"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-7dcc57f83bfc>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m def plot_data(train_ds=train_ds,\n\u001b[0m\u001b[1;32m      4\u001b[0m               \u001b[0mtest_ds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               \u001b[0mval_ds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_ds' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Model"
      ],
      "metadata": {
        "id": "J9k-LhbKXoqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define the Neural Network model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, input_size=14, hidden_size=90, output_size=1, num_hidden_layers=1, dropout_rate=0.2, activation=0):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Define the first layer\n",
        "        self.layers = [nn.Linear(in_features=input_size, out_features=hidden_size),\n",
        "                       self.get_activation(activation),\n",
        "                       nn.Dropout(p=dropout_rate)]\n",
        "\n",
        "        # Define hidden layers\n",
        "        for _ in range(num_hidden_layers):\n",
        "            self.layers.extend([nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
        "                               self.get_activation(activation),\n",
        "                               nn.Dropout(p=dropout_rate)])\n",
        "\n",
        "        # Define the output layer\n",
        "        self.layers.append(nn.Linear(in_features=hidden_size, out_features=output_size))\n",
        "\n",
        "        # Create the sequential model\n",
        "        self.model = nn.Sequential(*self.layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def get_activation(self, activation):\n",
        "        if activation == 0:\n",
        "            return nn.ReLU()\n",
        "        elif activation == 1:\n",
        "            return nn.LeakyReLU()\n",
        "        elif activation == 2:\n",
        "            return nn.Sigmoid()\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation function index: {activation}\")"
      ],
      "metadata": {
        "id": "hEemeW-IXqc7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "import torch\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model = Net()\n",
        "print(model)\n",
        "print(model.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKCfm-hcXzLE",
        "outputId": "78107e73-f769-4f38-8ed8-167da8212a24"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=14, out_features=90, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.2, inplace=False)\n",
            "    (3): Linear(in_features=90, out_features=90, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.2, inplace=False)\n",
            "    (6): Linear(in_features=90, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "OrderedDict([('model.0.weight', tensor([[ 0.2043,  0.2218, -0.0626,  ...,  0.0500,  0.1975,  0.0362],\n",
            "        [ 0.1289, -0.0377,  0.2060,  ..., -0.1232, -0.0755, -0.1607],\n",
            "        [ 0.0252, -0.2640,  0.2414,  ..., -0.0843,  0.0718, -0.0725],\n",
            "        ...,\n",
            "        [ 0.1289, -0.0934, -0.2331,  ..., -0.1677,  0.1301,  0.0455],\n",
            "        [ 0.0727,  0.0878,  0.2035,  ..., -0.0710,  0.1822,  0.0292],\n",
            "        [-0.2470, -0.0290, -0.1212,  ...,  0.1717,  0.0662, -0.2256]])), ('model.0.bias', tensor([ 0.0628,  0.2215, -0.1748, -0.1728,  0.2616,  0.2147, -0.1491,  0.1608,\n",
            "        -0.0342, -0.2635,  0.0201,  0.0864, -0.0802,  0.0930, -0.2285,  0.1852,\n",
            "         0.2278,  0.1474,  0.0453,  0.0880, -0.1934, -0.0668, -0.0255, -0.1487,\n",
            "        -0.1974,  0.1798,  0.1814, -0.2427,  0.0850,  0.1087,  0.2539,  0.1546,\n",
            "         0.2457, -0.0875,  0.1872,  0.2174, -0.2524, -0.2145,  0.0541,  0.1459,\n",
            "        -0.1328,  0.0294, -0.0004,  0.0676, -0.1436,  0.1507,  0.1777, -0.1854,\n",
            "         0.0026,  0.0007,  0.0858,  0.2490,  0.0784, -0.0607,  0.2400, -0.1017,\n",
            "        -0.0776,  0.0182, -0.2035,  0.0291, -0.1813,  0.0275,  0.0256,  0.0370,\n",
            "        -0.2254, -0.2538,  0.1230,  0.2292, -0.2372,  0.0990, -0.1967,  0.0168,\n",
            "         0.2486,  0.2027, -0.1954,  0.1653,  0.1396, -0.0004,  0.2589, -0.1061,\n",
            "        -0.0622, -0.1351,  0.0385,  0.0708,  0.0213, -0.1504, -0.1203,  0.0909,\n",
            "        -0.2343, -0.0416])), ('model.3.weight', tensor([[-0.0023, -0.0427,  0.0384,  ...,  0.0700, -0.0839,  0.0375],\n",
            "        [-0.0601, -0.0041, -0.0220,  ...,  0.0297,  0.0178, -0.0288],\n",
            "        [ 0.0826,  0.0633, -0.0655,  ..., -0.0127,  0.1031, -0.0228],\n",
            "        ...,\n",
            "        [-0.0600,  0.0626,  0.0939,  ..., -0.0388, -0.0740, -0.1002],\n",
            "        [ 0.0656,  0.0146, -0.0678,  ...,  0.0631,  0.0279,  0.0351],\n",
            "        [-0.0499,  0.0987,  0.0402,  ...,  0.0404, -0.0576,  0.1020]])), ('model.3.bias', tensor([-9.3156e-02,  5.8776e-02, -5.4730e-02, -3.7375e-02, -9.8425e-02,\n",
            "        -9.7693e-02, -9.9757e-02, -2.0246e-03,  9.7090e-02,  4.8332e-02,\n",
            "         6.6002e-02,  2.3602e-02, -4.0727e-02, -4.1052e-02, -3.1312e-02,\n",
            "        -9.8483e-02,  5.7941e-02, -3.1334e-02, -5.9626e-02,  2.6153e-02,\n",
            "         2.7016e-06,  5.7326e-02, -6.9030e-02,  1.0472e-01, -4.1959e-02,\n",
            "         3.9828e-02,  9.5960e-02, -6.0469e-03, -9.2625e-02, -1.0198e-01,\n",
            "         1.0369e-01, -9.8691e-02, -3.9103e-02,  7.1895e-02, -8.1988e-02,\n",
            "         9.8508e-02,  1.0237e-02,  3.2764e-02,  1.6540e-02,  1.1591e-02,\n",
            "         5.9159e-02, -9.1334e-02,  2.2786e-02, -4.7706e-02,  9.9761e-02,\n",
            "         8.7274e-02, -4.1407e-02,  5.8616e-02, -1.7950e-02,  5.5621e-02,\n",
            "        -2.9814e-02, -6.1685e-02,  4.4472e-02,  5.2192e-02,  9.4913e-02,\n",
            "         3.3406e-02, -4.7038e-02,  2.6931e-02,  6.3446e-03,  9.1701e-02,\n",
            "         4.3246e-02,  8.9906e-02,  7.8296e-03, -7.9909e-02, -6.7493e-02,\n",
            "         9.5178e-02,  9.4405e-02,  3.1753e-02, -7.5225e-02,  7.7025e-02,\n",
            "         4.3726e-02, -8.3173e-02,  3.0645e-02,  1.0160e-01, -6.9621e-02,\n",
            "         8.0108e-03,  3.0115e-02, -3.7939e-02,  6.9383e-02,  9.3739e-02,\n",
            "         1.0261e-01, -9.4346e-02,  5.8681e-03,  8.6042e-02,  8.9793e-02,\n",
            "        -2.7153e-02,  6.9830e-02,  7.8677e-02, -6.6675e-03,  7.2295e-02])), ('model.6.weight', tensor([[ 0.0606, -0.0157, -0.0440, -0.0559, -0.0849, -0.0630,  0.0275,  0.0376,\n",
            "         -0.0302, -0.0087,  0.0730,  0.0156, -0.0731,  0.0893,  0.0636,  0.0121,\n",
            "          0.0502, -0.0891,  0.0618,  0.0256, -0.0093, -0.0338,  0.0851,  0.0227,\n",
            "         -0.0589,  0.0474,  0.0186, -0.0370,  0.0744,  0.1050, -0.0047, -0.0435,\n",
            "         -0.0566, -0.0550,  0.0541,  0.0661, -0.0639,  0.0584,  0.0042, -0.0037,\n",
            "         -0.0335, -0.0971, -0.0904,  0.0773, -0.0173, -0.0046, -0.0636, -0.0192,\n",
            "         -0.0411,  0.0095, -0.1006,  0.0102, -0.0933, -0.0749, -0.0023, -0.0112,\n",
            "         -0.0296,  0.0613,  0.0076,  0.0218,  0.0431,  0.0060, -0.0934, -0.0392,\n",
            "          0.0662,  0.0605,  0.0365,  0.0930, -0.0548, -0.0314, -0.0157, -0.0259,\n",
            "          0.0909, -0.1053,  0.0200,  0.1035, -0.0586,  0.0838, -0.0908,  0.0178,\n",
            "         -0.0600,  0.0334,  0.0231,  0.0232, -0.0239, -0.0327, -0.1021,  0.0175,\n",
            "          0.1006, -0.0793]])), ('model.6.bias', tensor([-0.0406]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ],
      "metadata": {
        "id": "rVftevXtY9R2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def train_and_evaluate(model, loss_fn, optimizer, train_loader, val_loader, device):\n",
        "    num_epochs = 300\n",
        "    train_loss_values = []\n",
        "    test_loss_values = []\n",
        "    epoch_count = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Train Loop\n",
        "        model.train()\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            y_pred = model(inputs)\n",
        "            loss = loss_fn(y_pred, targets)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation Loop\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in test_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                test_pred = model(inputs)\n",
        "                test_loss = loss_fn(test_pred, targets)\n",
        "\n",
        "        # Print out what's happening\n",
        "        if epoch % 10 == 0:\n",
        "            epoch_count.append(epoch)\n",
        "            train_loss_values.append(loss.item())\n",
        "            test_loss_values.append(test_loss.item())\n",
        "            print(f\"Epoch: {epoch} | MSE Train Loss: {loss.item()} | MSE Test Loss: {test_loss.item()} \")\n",
        "\n",
        "    return epoch_count, train_loss_values, test_loss_values"
      ],
      "metadata": {
        "id": "UTbQAFrZZADU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Search"
      ],
      "metadata": {
        "id": "-JvcSe6eck1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize model folder\n",
        "%rm -r models"
      ],
      "metadata": {
        "id": "pcGp1WCUF8EQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from pathlib import Path\n",
        "\n",
        "def random_search(num_trials, search_space, output_size, device, train_loader, val_loader):\n",
        "    MODEL_PATH = Path(\"models\")\n",
        "    MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    best_losses = [float('inf')] * num_trials  # List to store the best losses\n",
        "    best_hyperparameters = [None] * num_trials\n",
        "    best_models = [None] * num_trials\n",
        "    trial_info = []\n",
        "\n",
        "    for trial in range(1, num_trials + 1):\n",
        "        # Randomly sample hyperparameters from the search space\n",
        "        hyperparameters = {\n",
        "            'hidden_size': torch.randint(search_space['hidden_size'][0], search_space['hidden_size'][1] + 1, (1,)).item(),\n",
        "            'learning_rate': torch.rand(1).item() * (search_space['learning_rate'][1] - search_space['learning_rate'][0]) + search_space['learning_rate'][0],\n",
        "            'num_hidden_layers': torch.randint(search_space['num_hidden_layers'][0], search_space['num_hidden_layers'][1] + 1, (1,)).item(),\n",
        "            'dropout_rate': torch.rand(1).item() * (search_space['dropout_rate'][1] - search_space['dropout_rate'][0]) + search_space['dropout_rate'][0],\n",
        "            'activation': torch.randint(0, len(search_space['activation']), (1,)).item()\n",
        "        }\n",
        "\n",
        "        # Create model\n",
        "        model = Net(input_size=14,\n",
        "                    hidden_size=hyperparameters['hidden_size'],\n",
        "                    output_size=output_size,\n",
        "                    num_hidden_layers=hyperparameters['num_hidden_layers'],\n",
        "                    dropout_rate=hyperparameters['dropout_rate'],\n",
        "                    activation=hyperparameters['activation'])\n",
        "        model.to(device)\n",
        "\n",
        "        # Define loss function, optimizer\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = optim.SGD(model.parameters(), lr=hyperparameters['learning_rate'])\n",
        "\n",
        "        # Train and evaluate the model\n",
        "        epoch_count, val_loss_values, test_loss_values = train_and_evaluate(model, criterion, optimizer, train_loader, val_loader, device)\n",
        "        val_loss = min(val_loss_values)\n",
        "        test_loss = min(test_loss_values)\n",
        "\n",
        "        # Update best hyperparameters and save the model if the current model is better\n",
        "        if val_loss < best_losses[trial - 1]:\n",
        "            best_losses[trial - 1] = val_loss\n",
        "            best_hyperparameters[trial - 1] = hyperparameters\n",
        "            best_models[trial - 1] = model\n",
        "            # Save the best model\n",
        "            MODEL_NAME = f\"best_model_{trial}.pth\"\n",
        "            MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "            torch.save(obj=model.state_dict(), f=MODEL_SAVE_PATH)\n",
        "            # Store trial information\n",
        "            trial_info.append({\n",
        "                'trial_number': trial,\n",
        "                'hyperparameters': hyperparameters,\n",
        "                'test_loss': test_loss,\n",
        "                'validation_loss': val_loss,\n",
        "                'model_path': MODEL_SAVE_PATH\n",
        "            })\n",
        "            # Print hyperparameters and validation loss for the current trial\n",
        "            print(f\"Hyperparameters for best_model_{trial}:\")\n",
        "            print(hyperparameters)\n",
        "            print(f\"Test Loss for best_model_{trial}: {test_loss}\")\n",
        "            print(f\"Validation Loss for best_model_{trial}: {val_loss}\")\n",
        "            print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
        "        print(f'Best hyperparameters: {best_hyperparameters}')\n",
        "        print(f'Best validation losses: {best_losses}')\n",
        "        print(\"\\n\")\n",
        "    return trial_info\n",
        "\n",
        "# Define search space for hyperparameters\n",
        "search_space = {\n",
        "    'hidden_size': (16, 256),\n",
        "    'learning_rate': (0.001, 0.1),\n",
        "    'num_hidden_layers': (1, 3),\n",
        "    'dropout_rate': (0.0, 0.5),\n",
        "    'activation': [0, 1, 2]\n",
        "}\n",
        "\n",
        "# Model parameters\n",
        "output_size = 1\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 64\n",
        "\n",
        "# Number of random trials\n",
        "num_trials = 8\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Perform random search\n",
        "trial_info = random_search(num_trials, search_space, output_size, device, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5yN-3D-Fdmw",
        "outputId": "003139a4-da04-4905-e1d9-42fe9af29f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | MSE Train Loss: 0.16283878684043884 | MSE Test Loss: 0.009976253844797611 \n",
            "Epoch: 10 | MSE Train Loss: 0.006063547916710377 | MSE Test Loss: 0.008057867176830769 \n",
            "Epoch: 20 | MSE Train Loss: 0.029780155047774315 | MSE Test Loss: 0.006031943950802088 \n",
            "Epoch: 30 | MSE Train Loss: 0.014052381739020348 | MSE Test Loss: 0.01035288441926241 \n",
            "Epoch: 40 | MSE Train Loss: 0.2877591550350189 | MSE Test Loss: 0.017768345773220062 \n",
            "Epoch: 50 | MSE Train Loss: 0.11783970892429352 | MSE Test Loss: 0.013479674234986305 \n",
            "Epoch: 60 | MSE Train Loss: 0.2400197982788086 | MSE Test Loss: 0.009183242917060852 \n",
            "Epoch: 70 | MSE Train Loss: 0.03327701613306999 | MSE Test Loss: 0.011205746792256832 \n",
            "Epoch: 80 | MSE Train Loss: 0.016698170453310013 | MSE Test Loss: 0.006459665484726429 \n",
            "Epoch: 90 | MSE Train Loss: 0.015470175072550774 | MSE Test Loss: 0.008945454843342304 \n",
            "Epoch: 100 | MSE Train Loss: 0.018296588212251663 | MSE Test Loss: 0.008880441077053547 \n",
            "Epoch: 110 | MSE Train Loss: 0.013445769436657429 | MSE Test Loss: 0.00860634259879589 \n",
            "Epoch: 120 | MSE Train Loss: 0.007136260159313679 | MSE Test Loss: 0.007482065353542566 \n",
            "Epoch: 130 | MSE Train Loss: 0.10625724494457245 | MSE Test Loss: 0.011671398766338825 \n",
            "Epoch: 140 | MSE Train Loss: 0.022907797247171402 | MSE Test Loss: 0.008126134052872658 \n",
            "Epoch: 150 | MSE Train Loss: 0.01834677718579769 | MSE Test Loss: 0.0070992279797792435 \n",
            "Epoch: 160 | MSE Train Loss: 0.0059849913232028484 | MSE Test Loss: 0.010133668780326843 \n",
            "Epoch: 170 | MSE Train Loss: 0.02307712286710739 | MSE Test Loss: 0.010215183719992638 \n",
            "Epoch: 180 | MSE Train Loss: 0.02042440138757229 | MSE Test Loss: 0.008816084824502468 \n",
            "Epoch: 190 | MSE Train Loss: 0.14164778590202332 | MSE Test Loss: 0.009887482970952988 \n",
            "Epoch: 200 | MSE Train Loss: 0.012871116399765015 | MSE Test Loss: 0.006526219192892313 \n",
            "Epoch: 210 | MSE Train Loss: 0.04493599385023117 | MSE Test Loss: 0.009770055301487446 \n",
            "Epoch: 220 | MSE Train Loss: 0.1463421881198883 | MSE Test Loss: 0.011191233061254025 \n",
            "Epoch: 230 | MSE Train Loss: 0.024631410837173462 | MSE Test Loss: 0.008639699779450893 \n",
            "Epoch: 240 | MSE Train Loss: 0.015531140379607677 | MSE Test Loss: 0.006595786195248365 \n",
            "Epoch: 250 | MSE Train Loss: 0.059417229145765305 | MSE Test Loss: 0.00519484793767333 \n",
            "Epoch: 260 | MSE Train Loss: 0.00836196355521679 | MSE Test Loss: 0.008086036890745163 \n",
            "Epoch: 270 | MSE Train Loss: 0.030182788148522377 | MSE Test Loss: 0.010187836363911629 \n",
            "Epoch: 280 | MSE Train Loss: 0.0864069014787674 | MSE Test Loss: 0.007089806254953146 \n",
            "Epoch: 290 | MSE Train Loss: 0.025714024901390076 | MSE Test Loss: 0.008113878779113293 \n",
            "Hyperparameters for best_model_1:\n",
            "{'hidden_size': 250, 'learning_rate': 0.07581899631023407, 'num_hidden_layers': 1, 'dropout_rate': 0.4119832515716553, 'activation': 2}\n",
            "Validation Loss for best_model_1: 0.0059849913232028484\n",
            "Saving model to: models/best_model_1.pth\n",
            "Best hyperparameters: [{'hidden_size': 250, 'learning_rate': 0.07581899631023407, 'num_hidden_layers': 1, 'dropout_rate': 0.4119832515716553, 'activation': 2}, None, None, None, None, None, None, None]\n",
            "Best validation losses: [0.0059849913232028484, inf, inf, inf, inf, inf, inf, inf]\n",
            "\n",
            "\n",
            "Epoch: 0 | MSE Train Loss: 0.030312035232782364 | MSE Test Loss: 0.023877525702118874 \n",
            "Epoch: 10 | MSE Train Loss: 0.026755675673484802 | MSE Test Loss: 0.035152606666088104 \n",
            "Epoch: 20 | MSE Train Loss: 0.06346612423658371 | MSE Test Loss: 0.0031070916447788477 \n",
            "Epoch: 30 | MSE Train Loss: 0.06087563559412956 | MSE Test Loss: 0.003188458038493991 \n",
            "Epoch: 40 | MSE Train Loss: 0.01606668531894684 | MSE Test Loss: 0.02526226080954075 \n",
            "Epoch: 50 | MSE Train Loss: 0.023707114160060883 | MSE Test Loss: 0.005669944453984499 \n",
            "Epoch: 60 | MSE Train Loss: 0.020191529765725136 | MSE Test Loss: 0.004777905531227589 \n",
            "Epoch: 70 | MSE Train Loss: 0.07432247698307037 | MSE Test Loss: 0.005681775044649839 \n",
            "Epoch: 80 | MSE Train Loss: 0.06593917310237885 | MSE Test Loss: 0.016509123146533966 \n",
            "Epoch: 90 | MSE Train Loss: 0.008251158520579338 | MSE Test Loss: 0.006785932462662458 \n",
            "Epoch: 100 | MSE Train Loss: 0.5264989137649536 | MSE Test Loss: 0.1385679990053177 \n",
            "Epoch: 110 | MSE Train Loss: 0.1801636815071106 | MSE Test Loss: 0.0033770990557968616 \n",
            "Epoch: 120 | MSE Train Loss: 0.024017462506890297 | MSE Test Loss: 0.003139277920126915 \n",
            "Epoch: 130 | MSE Train Loss: 0.016511326655745506 | MSE Test Loss: 0.004094937350600958 \n",
            "Epoch: 140 | MSE Train Loss: 0.0870400220155716 | MSE Test Loss: 0.003529777517542243 \n",
            "Epoch: 150 | MSE Train Loss: 0.3209967315196991 | MSE Test Loss: 0.03281597048044205 \n",
            "Epoch: 160 | MSE Train Loss: 0.05139078572392464 | MSE Test Loss: 0.024653693661093712 \n",
            "Epoch: 170 | MSE Train Loss: 0.02979794144630432 | MSE Test Loss: 0.0077728466130793095 \n",
            "Epoch: 180 | MSE Train Loss: 0.04351181909441948 | MSE Test Loss: 0.014623074792325497 \n",
            "Epoch: 190 | MSE Train Loss: 0.0082557313144207 | MSE Test Loss: 0.005418727640062571 \n",
            "Epoch: 200 | MSE Train Loss: 0.018440406769514084 | MSE Test Loss: 0.011719518341124058 \n",
            "Epoch: 210 | MSE Train Loss: 0.07226289063692093 | MSE Test Loss: 0.0032515465281903744 \n",
            "Epoch: 220 | MSE Train Loss: 0.41982898116111755 | MSE Test Loss: 0.0520283579826355 \n",
            "Epoch: 230 | MSE Train Loss: 0.006929428782314062 | MSE Test Loss: 0.006560875568538904 \n",
            "Epoch: 240 | MSE Train Loss: 0.0716206505894661 | MSE Test Loss: 0.022455638274550438 \n",
            "Epoch: 250 | MSE Train Loss: 0.06280051171779633 | MSE Test Loss: 0.0031131692230701447 \n",
            "Epoch: 260 | MSE Train Loss: 0.07799850404262543 | MSE Test Loss: 0.0034952841233462095 \n",
            "Epoch: 270 | MSE Train Loss: 0.4644704759120941 | MSE Test Loss: 0.06713207066059113 \n",
            "Epoch: 280 | MSE Train Loss: 0.15976294875144958 | MSE Test Loss: 0.00720612145960331 \n",
            "Epoch: 290 | MSE Train Loss: 0.057699937373399734 | MSE Test Loss: 0.012150689959526062 \n",
            "Hyperparameters for best_model_2:\n",
            "{'hidden_size': 155, 'learning_rate': 0.010746054828166964, 'num_hidden_layers': 2, 'dropout_rate': 0.0444769561290741, 'activation': 2}\n",
            "Validation Loss for best_model_2: 0.006929428782314062\n",
            "Saving model to: models/best_model_2.pth\n",
            "Best hyperparameters: [{'hidden_size': 250, 'learning_rate': 0.07581899631023407, 'num_hidden_layers': 1, 'dropout_rate': 0.4119832515716553, 'activation': 2}, {'hidden_size': 155, 'learning_rate': 0.010746054828166964, 'num_hidden_layers': 2, 'dropout_rate': 0.0444769561290741, 'activation': 2}, None, None, None, None, None, None]\n",
            "Best validation losses: [0.0059849913232028484, 0.006929428782314062, inf, inf, inf, inf, inf, inf]\n",
            "\n",
            "\n",
            "Epoch: 0 | MSE Train Loss: 0.1423387974500656 | MSE Test Loss: 0.010737894102931023 \n",
            "Epoch: 10 | MSE Train Loss: 0.0742475837469101 | MSE Test Loss: 0.0059715090319514275 \n",
            "Epoch: 20 | MSE Train Loss: 0.025024589151144028 | MSE Test Loss: 0.012035273015499115 \n",
            "Epoch: 30 | MSE Train Loss: 0.008596519008278847 | MSE Test Loss: 0.005241816863417625 \n",
            "Epoch: 40 | MSE Train Loss: 0.11729644238948822 | MSE Test Loss: 0.008570357225835323 \n",
            "Epoch: 50 | MSE Train Loss: 0.015902917832136154 | MSE Test Loss: 0.01596923917531967 \n",
            "Epoch: 60 | MSE Train Loss: 0.02332601509988308 | MSE Test Loss: 0.005339472554624081 \n",
            "Epoch: 70 | MSE Train Loss: 0.01562957651913166 | MSE Test Loss: 0.004520382732152939 \n",
            "Epoch: 80 | MSE Train Loss: 0.004834512248635292 | MSE Test Loss: 0.007834353484213352 \n",
            "Epoch: 90 | MSE Train Loss: 0.06762059032917023 | MSE Test Loss: 0.006637656595557928 \n",
            "Epoch: 100 | MSE Train Loss: 0.008286044001579285 | MSE Test Loss: 0.004904475063085556 \n",
            "Epoch: 110 | MSE Train Loss: 0.28740158677101135 | MSE Test Loss: 0.10716574639081955 \n",
            "Epoch: 120 | MSE Train Loss: 0.01053941622376442 | MSE Test Loss: 0.004389125853776932 \n",
            "Epoch: 130 | MSE Train Loss: 0.00569855235517025 | MSE Test Loss: 0.010828835889697075 \n",
            "Epoch: 140 | MSE Train Loss: 0.02639974094927311 | MSE Test Loss: 0.010050565004348755 \n",
            "Epoch: 150 | MSE Train Loss: 0.014631216414272785 | MSE Test Loss: 0.010563673451542854 \n",
            "Epoch: 160 | MSE Train Loss: 0.07682359218597412 | MSE Test Loss: 0.0047445716336369514 \n",
            "Epoch: 170 | MSE Train Loss: 0.006997962482273579 | MSE Test Loss: 0.011635974049568176 \n",
            "Epoch: 180 | MSE Train Loss: 0.01247780304402113 | MSE Test Loss: 0.004081288352608681 \n",
            "Epoch: 190 | MSE Train Loss: 0.019709840416908264 | MSE Test Loss: 0.00737233180552721 \n",
            "Epoch: 200 | MSE Train Loss: 0.027328014373779297 | MSE Test Loss: 0.00569506362080574 \n",
            "Epoch: 210 | MSE Train Loss: 0.04583078622817993 | MSE Test Loss: 0.014196999371051788 \n",
            "Epoch: 220 | MSE Train Loss: 0.0530686192214489 | MSE Test Loss: 0.006420581601560116 \n",
            "Epoch: 230 | MSE Train Loss: 0.019451472908258438 | MSE Test Loss: 0.00932512991130352 \n",
            "Epoch: 240 | MSE Train Loss: 0.032068148255348206 | MSE Test Loss: 0.006599878426641226 \n",
            "Epoch: 250 | MSE Train Loss: 0.009835564531385899 | MSE Test Loss: 0.0033120603766292334 \n",
            "Epoch: 260 | MSE Train Loss: 0.21318545937538147 | MSE Test Loss: 0.0034472369588911533 \n",
            "Epoch: 270 | MSE Train Loss: 0.07345167547464371 | MSE Test Loss: 0.06171168014407158 \n",
            "Epoch: 280 | MSE Train Loss: 0.014848371967673302 | MSE Test Loss: 0.0059073306620121 \n",
            "Epoch: 290 | MSE Train Loss: 0.08272397518157959 | MSE Test Loss: 0.01605023629963398 \n",
            "Hyperparameters for best_model_3:\n",
            "{'hidden_size': 213, 'learning_rate': 0.09934136962890626, 'num_hidden_layers': 3, 'dropout_rate': 0.030684202909469604, 'activation': 1}\n",
            "Validation Loss for best_model_3: 0.004834512248635292\n",
            "Saving model to: models/best_model_3.pth\n",
            "Best hyperparameters: [{'hidden_size': 250, 'learning_rate': 0.07581899631023407, 'num_hidden_layers': 1, 'dropout_rate': 0.4119832515716553, 'activation': 2}, {'hidden_size': 155, 'learning_rate': 0.010746054828166964, 'num_hidden_layers': 2, 'dropout_rate': 0.0444769561290741, 'activation': 2}, {'hidden_size': 213, 'learning_rate': 0.09934136962890626, 'num_hidden_layers': 3, 'dropout_rate': 0.030684202909469604, 'activation': 1}, None, None, None, None, None]\n",
            "Best validation losses: [0.0059849913232028484, 0.006929428782314062, 0.004834512248635292, inf, inf, inf, inf, inf]\n",
            "\n",
            "\n",
            "Epoch: 0 | MSE Train Loss: 0.016388526186347008 | MSE Test Loss: 0.012461368925869465 \n",
            "Epoch: 10 | MSE Train Loss: 0.02038080431520939 | MSE Test Loss: 0.00844612717628479 \n",
            "Epoch: 20 | MSE Train Loss: 0.021432599052786827 | MSE Test Loss: 0.007595013827085495 \n",
            "Epoch: 30 | MSE Train Loss: 0.01839166134595871 | MSE Test Loss: 0.014085819013416767 \n",
            "Epoch: 40 | MSE Train Loss: 0.005206795409321785 | MSE Test Loss: 0.016397625207901 \n",
            "Epoch: 50 | MSE Train Loss: 0.025719892233610153 | MSE Test Loss: 0.011351171880960464 \n",
            "Epoch: 60 | MSE Train Loss: 0.01143029797822237 | MSE Test Loss: 0.007291655987501144 \n",
            "Epoch: 70 | MSE Train Loss: 0.14723819494247437 | MSE Test Loss: 0.021292222663760185 \n",
            "Epoch: 80 | MSE Train Loss: 0.011247102171182632 | MSE Test Loss: 0.009383090771734715 \n",
            "Epoch: 90 | MSE Train Loss: 0.02665891870856285 | MSE Test Loss: 0.00536968233063817 \n",
            "Epoch: 100 | MSE Train Loss: 0.027276750653982162 | MSE Test Loss: 0.018639210611581802 \n",
            "Epoch: 110 | MSE Train Loss: 0.03181241825222969 | MSE Test Loss: 0.012624361552298069 \n",
            "Epoch: 120 | MSE Train Loss: 0.008880721405148506 | MSE Test Loss: 0.010798292234539986 \n",
            "Epoch: 130 | MSE Train Loss: 0.02162356860935688 | MSE Test Loss: 0.009905590675771236 \n",
            "Epoch: 140 | MSE Train Loss: 0.18612949550151825 | MSE Test Loss: 0.01143736857920885 \n",
            "Epoch: 150 | MSE Train Loss: 0.0195433609187603 | MSE Test Loss: 0.01203327439725399 \n",
            "Epoch: 160 | MSE Train Loss: 0.10042271763086319 | MSE Test Loss: 0.013084329664707184 \n",
            "Epoch: 170 | MSE Train Loss: 0.02455405704677105 | MSE Test Loss: 0.006323852110654116 \n",
            "Epoch: 180 | MSE Train Loss: 0.015531674027442932 | MSE Test Loss: 0.007349198218435049 \n",
            "Epoch: 190 | MSE Train Loss: 0.09803177416324615 | MSE Test Loss: 0.0032475097104907036 \n",
            "Epoch: 200 | MSE Train Loss: 0.008240146562457085 | MSE Test Loss: 0.004388106521219015 \n",
            "Epoch: 210 | MSE Train Loss: 0.02076077088713646 | MSE Test Loss: 0.007088031619787216 \n",
            "Epoch: 220 | MSE Train Loss: 0.01678560860455036 | MSE Test Loss: 0.00804413203150034 \n",
            "Epoch: 230 | MSE Train Loss: 0.020282799378037453 | MSE Test Loss: 0.007258924189954996 \n",
            "Epoch: 240 | MSE Train Loss: 0.029057443141937256 | MSE Test Loss: 0.004808146972209215 \n",
            "Epoch: 250 | MSE Train Loss: 0.047245871275663376 | MSE Test Loss: 0.004513280466198921 \n",
            "Epoch: 260 | MSE Train Loss: 0.08462897688150406 | MSE Test Loss: 0.0033421048428863287 \n",
            "Epoch: 270 | MSE Train Loss: 0.011024018749594688 | MSE Test Loss: 0.005814571864902973 \n",
            "Epoch: 280 | MSE Train Loss: 0.023045262321829796 | MSE Test Loss: 0.0054853432811796665 \n",
            "Epoch: 290 | MSE Train Loss: 0.019535917788743973 | MSE Test Loss: 0.012665772810578346 \n",
            "Hyperparameters for best_model_4:\n",
            "{'hidden_size': 64, 'learning_rate': 0.07174886053800583, 'num_hidden_layers': 3, 'dropout_rate': 0.10007864236831665, 'activation': 1}\n",
            "Validation Loss for best_model_4: 0.005206795409321785\n",
            "Saving model to: models/best_model_4.pth\n",
            "Best hyperparameters: [{'hidden_size': 250, 'learning_rate': 0.07581899631023407, 'num_hidden_layers': 1, 'dropout_rate': 0.4119832515716553, 'activation': 2}, {'hidden_size': 155, 'learning_rate': 0.010746054828166964, 'num_hidden_layers': 2, 'dropout_rate': 0.0444769561290741, 'activation': 2}, {'hidden_size': 213, 'learning_rate': 0.09934136962890626, 'num_hidden_layers': 3, 'dropout_rate': 0.030684202909469604, 'activation': 1}, {'hidden_size': 64, 'learning_rate': 0.07174886053800583, 'num_hidden_layers': 3, 'dropout_rate': 0.10007864236831665, 'activation': 1}, None, None, None, None]\n",
            "Best validation losses: [0.0059849913232028484, 0.006929428782314062, 0.004834512248635292, 0.005206795409321785, inf, inf, inf, inf]\n",
            "\n",
            "\n",
            "Epoch: 0 | MSE Train Loss: 0.050774503499269485 | MSE Test Loss: 0.010908182710409164 \n",
            "Epoch: 10 | MSE Train Loss: 0.2807212471961975 | MSE Test Loss: 0.008745314553380013 \n",
            "Epoch: 20 | MSE Train Loss: 0.017096644267439842 | MSE Test Loss: 0.006631721276789904 \n",
            "Epoch: 30 | MSE Train Loss: 0.04072839766740799 | MSE Test Loss: 0.006820296868681908 \n",
            "Epoch: 40 | MSE Train Loss: 0.15326030552387238 | MSE Test Loss: 0.01001336146146059 \n",
            "Epoch: 50 | MSE Train Loss: 0.01148330420255661 | MSE Test Loss: 0.007519060280174017 \n",
            "Epoch: 60 | MSE Train Loss: 0.07517587393522263 | MSE Test Loss: 0.007808425463736057 \n",
            "Epoch: 70 | MSE Train Loss: 0.196323424577713 | MSE Test Loss: 0.011059651151299477 \n",
            "Epoch: 80 | MSE Train Loss: 0.02844257652759552 | MSE Test Loss: 0.007858327589929104 \n",
            "Epoch: 90 | MSE Train Loss: 0.012663083150982857 | MSE Test Loss: 0.0073751723393797874 \n",
            "Epoch: 100 | MSE Train Loss: 0.006397376302629709 | MSE Test Loss: 0.008555902168154716 \n",
            "Epoch: 110 | MSE Train Loss: 0.07317455857992172 | MSE Test Loss: 0.006501464638859034 \n",
            "Epoch: 120 | MSE Train Loss: 0.015084612183272839 | MSE Test Loss: 0.009933622553944588 \n",
            "Epoch: 130 | MSE Train Loss: 0.35856130719184875 | MSE Test Loss: 0.016491010785102844 \n",
            "Epoch: 140 | MSE Train Loss: 0.05566296726465225 | MSE Test Loss: 0.01259602326899767 \n",
            "Epoch: 150 | MSE Train Loss: 0.01833319664001465 | MSE Test Loss: 0.010046270675957203 \n",
            "Epoch: 160 | MSE Train Loss: 0.018457289785146713 | MSE Test Loss: 0.010994198732078075 \n",
            "Epoch: 170 | MSE Train Loss: 0.018496932461857796 | MSE Test Loss: 0.011610167101025581 \n",
            "Epoch: 180 | MSE Train Loss: 0.029161591082811356 | MSE Test Loss: 0.010269210673868656 \n",
            "Epoch: 190 | MSE Train Loss: 0.02794336900115013 | MSE Test Loss: 0.012790833599865437 \n",
            "Epoch: 200 | MSE Train Loss: 0.10959514230489731 | MSE Test Loss: 0.009391838684678078 \n",
            "Epoch: 210 | MSE Train Loss: 0.05900895595550537 | MSE Test Loss: 0.015056238509714603 \n",
            "Epoch: 220 | MSE Train Loss: 0.015416368842124939 | MSE Test Loss: 0.014590142294764519 \n",
            "Epoch: 230 | MSE Train Loss: 0.3603687286376953 | MSE Test Loss: 0.017092686146497726 \n",
            "Epoch: 240 | MSE Train Loss: 0.5684378743171692 | MSE Test Loss: 0.014681443572044373 \n",
            "Epoch: 250 | MSE Train Loss: 0.010744298808276653 | MSE Test Loss: 0.012070978991687298 \n",
            "Epoch: 260 | MSE Train Loss: 0.006707610096782446 | MSE Test Loss: 0.01519443653523922 \n",
            "Epoch: 270 | MSE Train Loss: 0.09974122047424316 | MSE Test Loss: 0.012587001547217369 \n",
            "Epoch: 280 | MSE Train Loss: 0.06459860503673553 | MSE Test Loss: 0.009949487634003162 \n",
            "Epoch: 290 | MSE Train Loss: 0.13699795305728912 | MSE Test Loss: 0.014773779548704624 \n",
            "Hyperparameters for best_model_5:\n",
            "{'hidden_size': 72, 'learning_rate': 0.04337182623147965, 'num_hidden_layers': 2, 'dropout_rate': 0.45266228914260864, 'activation': 1}\n",
            "Validation Loss for best_model_5: 0.006397376302629709\n",
            "Saving model to: models/best_model_5.pth\n",
            "Best hyperparameters: [{'hidden_size': 250, 'learning_rate': 0.07581899631023407, 'num_hidden_layers': 1, 'dropout_rate': 0.4119832515716553, 'activation': 2}, {'hidden_size': 155, 'learning_rate': 0.010746054828166964, 'num_hidden_layers': 2, 'dropout_rate': 0.0444769561290741, 'activation': 2}, {'hidden_size': 213, 'learning_rate': 0.09934136962890626, 'num_hidden_layers': 3, 'dropout_rate': 0.030684202909469604, 'activation': 1}, {'hidden_size': 64, 'learning_rate': 0.07174886053800583, 'num_hidden_layers': 3, 'dropout_rate': 0.10007864236831665, 'activation': 1}, {'hidden_size': 72, 'learning_rate': 0.04337182623147965, 'num_hidden_layers': 2, 'dropout_rate': 0.45266228914260864, 'activation': 1}, None, None, None]\n",
            "Best validation losses: [0.0059849913232028484, 0.006929428782314062, 0.004834512248635292, 0.005206795409321785, 0.006397376302629709, inf, inf, inf]\n",
            "\n",
            "\n",
            "Epoch: 0 | MSE Train Loss: 0.048930633813142776 | MSE Test Loss: 0.0060297828167676926 \n",
            "Epoch: 10 | MSE Train Loss: 0.035028476268053055 | MSE Test Loss: 0.0033295785542577505 \n",
            "Epoch: 20 | MSE Train Loss: 0.0716223493218422 | MSE Test Loss: 0.009108608588576317 \n",
            "Epoch: 30 | MSE Train Loss: 0.06471145153045654 | MSE Test Loss: 0.0031097400933504105 \n",
            "Epoch: 40 | MSE Train Loss: 0.023940395563840866 | MSE Test Loss: 0.0033349492587149143 \n",
            "Epoch: 50 | MSE Train Loss: 0.04583416134119034 | MSE Test Loss: 0.01626795344054699 \n",
            "Epoch: 60 | MSE Train Loss: 0.060457438230514526 | MSE Test Loss: 0.034119319170713425 \n",
            "Epoch: 70 | MSE Train Loss: 0.007984990254044533 | MSE Test Loss: 0.008274637162685394 \n",
            "Epoch: 80 | MSE Train Loss: 0.06741975247859955 | MSE Test Loss: 0.04230152443051338 \n",
            "Epoch: 90 | MSE Train Loss: 0.325295627117157 | MSE Test Loss: 0.028287513181567192 \n",
            "Epoch: 100 | MSE Train Loss: 0.3662239611148834 | MSE Test Loss: 0.03150377422571182 \n",
            "Epoch: 110 | MSE Train Loss: 0.06386862695217133 | MSE Test Loss: 0.003964542411267757 \n",
            "Epoch: 120 | MSE Train Loss: 0.007975280284881592 | MSE Test Loss: 0.005307003390043974 \n",
            "Epoch: 130 | MSE Train Loss: 0.3780992031097412 | MSE Test Loss: 0.06002082675695419 \n",
            "Epoch: 140 | MSE Train Loss: 0.0950397327542305 | MSE Test Loss: 0.04628884419798851 \n",
            "Epoch: 150 | MSE Train Loss: 0.029675951227545738 | MSE Test Loss: 0.008021463640034199 \n",
            "Epoch: 160 | MSE Train Loss: 0.12031915038824081 | MSE Test Loss: 0.01247403770685196 \n",
            "Epoch: 170 | MSE Train Loss: 0.10221616923809052 | MSE Test Loss: 0.008753428235650063 \n",
            "Epoch: 180 | MSE Train Loss: 0.008475743234157562 | MSE Test Loss: 0.0062276399694383144 \n",
            "Epoch: 190 | MSE Train Loss: 0.00940996315330267 | MSE Test Loss: 0.006792599800974131 \n",
            "Epoch: 200 | MSE Train Loss: 0.023133937269449234 | MSE Test Loss: 0.006154379807412624 \n",
            "Epoch: 210 | MSE Train Loss: 0.029226522892713547 | MSE Test Loss: 0.007113330997526646 \n",
            "Epoch: 220 | MSE Train Loss: 0.022476663812994957 | MSE Test Loss: 0.011521097272634506 \n",
            "Epoch: 230 | MSE Train Loss: 0.06315022706985474 | MSE Test Loss: 0.003191145369783044 \n",
            "Epoch: 240 | MSE Train Loss: 0.060818254947662354 | MSE Test Loss: 0.017444035038352013 \n",
            "Epoch: 250 | MSE Train Loss: 0.08612821996212006 | MSE Test Loss: 0.0036854923237115145 \n",
            "Epoch: 260 | MSE Train Loss: 0.5328234434127808 | MSE Test Loss: 0.042768444865942 \n",
            "Epoch: 270 | MSE Train Loss: 0.025410562753677368 | MSE Test Loss: 0.003727186471223831 \n",
            "Epoch: 280 | MSE Train Loss: 0.012938323430716991 | MSE Test Loss: 0.010777329094707966 \n",
            "Epoch: 290 | MSE Train Loss: 0.08197115361690521 | MSE Test Loss: 0.016078924760222435 \n",
            "Hyperparameters for best_model_6:\n",
            "{'hidden_size': 21, 'learning_rate': 0.07325429970026016, 'num_hidden_layers': 3, 'dropout_rate': 0.4275458753108978, 'activation': 2}\n",
            "Validation Loss for best_model_6: 0.007975280284881592\n",
            "Saving model to: models/best_model_6.pth\n",
            "Best hyperparameters: [{'hidden_size': 250, 'learning_rate': 0.07581899631023407, 'num_hidden_layers': 1, 'dropout_rate': 0.4119832515716553, 'activation': 2}, {'hidden_size': 155, 'learning_rate': 0.010746054828166964, 'num_hidden_layers': 2, 'dropout_rate': 0.0444769561290741, 'activation': 2}, {'hidden_size': 213, 'learning_rate': 0.09934136962890626, 'num_hidden_layers': 3, 'dropout_rate': 0.030684202909469604, 'activation': 1}, {'hidden_size': 64, 'learning_rate': 0.07174886053800583, 'num_hidden_layers': 3, 'dropout_rate': 0.10007864236831665, 'activation': 1}, {'hidden_size': 72, 'learning_rate': 0.04337182623147965, 'num_hidden_layers': 2, 'dropout_rate': 0.45266228914260864, 'activation': 1}, {'hidden_size': 21, 'learning_rate': 0.07325429970026016, 'num_hidden_layers': 3, 'dropout_rate': 0.4275458753108978, 'activation': 2}, None, None]\n",
            "Best validation losses: [0.0059849913232028484, 0.006929428782314062, 0.004834512248635292, 0.005206795409321785, 0.006397376302629709, 0.007975280284881592, inf, inf]\n",
            "\n",
            "\n",
            "Epoch: 0 | MSE Train Loss: 0.015411789529025555 | MSE Test Loss: 0.0065589407458901405 \n",
            "Epoch: 10 | MSE Train Loss: 0.05508090928196907 | MSE Test Loss: 0.013200100511312485 \n",
            "Epoch: 20 | MSE Train Loss: 0.009558014571666718 | MSE Test Loss: 0.012134824879467487 \n",
            "Epoch: 30 | MSE Train Loss: 0.047542035579681396 | MSE Test Loss: 0.009697234258055687 \n",
            "Epoch: 40 | MSE Train Loss: 0.020559106022119522 | MSE Test Loss: 0.008510429412126541 \n",
            "Epoch: 50 | MSE Train Loss: 0.025860082358121872 | MSE Test Loss: 0.007629132363945246 \n",
            "Epoch: 60 | MSE Train Loss: 0.014765920117497444 | MSE Test Loss: 0.019959785044193268 \n",
            "Epoch: 70 | MSE Train Loss: 0.033818550407886505 | MSE Test Loss: 0.019793009385466576 \n",
            "Epoch: 80 | MSE Train Loss: 0.022827045992016792 | MSE Test Loss: 0.016731897369027138 \n",
            "Epoch: 90 | MSE Train Loss: 0.012715649791061878 | MSE Test Loss: 0.013432634994387627 \n",
            "Epoch: 100 | MSE Train Loss: 0.010703555308282375 | MSE Test Loss: 0.018296558409929276 \n",
            "Epoch: 110 | MSE Train Loss: 0.0160723514854908 | MSE Test Loss: 0.014517450705170631 \n",
            "Epoch: 120 | MSE Train Loss: 0.011907282285392284 | MSE Test Loss: 0.01614220254123211 \n",
            "Epoch: 130 | MSE Train Loss: 0.24309416115283966 | MSE Test Loss: 0.018323689699172974 \n",
            "Epoch: 140 | MSE Train Loss: 0.02494415082037449 | MSE Test Loss: 0.011928878724575043 \n",
            "Epoch: 150 | MSE Train Loss: 0.047434866428375244 | MSE Test Loss: 0.01563207432627678 \n",
            "Epoch: 160 | MSE Train Loss: 0.021047797054052353 | MSE Test Loss: 0.011436723172664642 \n",
            "Epoch: 170 | MSE Train Loss: 0.0938480794429779 | MSE Test Loss: 0.010988905094563961 \n",
            "Epoch: 180 | MSE Train Loss: 0.05379311367869377 | MSE Test Loss: 0.01649908721446991 \n",
            "Epoch: 190 | MSE Train Loss: 0.03151318430900574 | MSE Test Loss: 0.013611948117613792 \n",
            "Epoch: 200 | MSE Train Loss: 0.029862582683563232 | MSE Test Loss: 0.011391397565603256 \n",
            "Epoch: 210 | MSE Train Loss: 0.41194844245910645 | MSE Test Loss: 0.0218052938580513 \n",
            "Epoch: 220 | MSE Train Loss: 0.09693875163793564 | MSE Test Loss: 0.011016249656677246 \n",
            "Epoch: 230 | MSE Train Loss: 0.021620918065309525 | MSE Test Loss: 0.014756654389202595 \n",
            "Epoch: 240 | MSE Train Loss: 0.01937638409435749 | MSE Test Loss: 0.013115264475345612 \n",
            "Epoch: 250 | MSE Train Loss: 0.0246963519603014 | MSE Test Loss: 0.014276476576924324 \n",
            "Epoch: 260 | MSE Train Loss: 0.05270541459321976 | MSE Test Loss: 0.013002823106944561 \n",
            "Epoch: 270 | MSE Train Loss: 0.015042273327708244 | MSE Test Loss: 0.009303980506956577 \n",
            "Epoch: 280 | MSE Train Loss: 0.04923795536160469 | MSE Test Loss: 0.010551218874752522 \n",
            "Epoch: 290 | MSE Train Loss: 0.018204450607299805 | MSE Test Loss: 0.008693639189004898 \n",
            "Hyperparameters for best_model_7:\n",
            "{'hidden_size': 39, 'learning_rate': 0.09714972537755967, 'num_hidden_layers': 1, 'dropout_rate': 0.4642423987388611, 'activation': 1}\n",
            "Validation Loss for best_model_7: 0.009558014571666718\n",
            "Saving model to: models/best_model_7.pth\n",
            "Best hyperparameters: [{'hidden_size': 250, 'learning_rate': 0.07581899631023407, 'num_hidden_layers': 1, 'dropout_rate': 0.4119832515716553, 'activation': 2}, {'hidden_size': 155, 'learning_rate': 0.010746054828166964, 'num_hidden_layers': 2, 'dropout_rate': 0.0444769561290741, 'activation': 2}, {'hidden_size': 213, 'learning_rate': 0.09934136962890626, 'num_hidden_layers': 3, 'dropout_rate': 0.030684202909469604, 'activation': 1}, {'hidden_size': 64, 'learning_rate': 0.07174886053800583, 'num_hidden_layers': 3, 'dropout_rate': 0.10007864236831665, 'activation': 1}, {'hidden_size': 72, 'learning_rate': 0.04337182623147965, 'num_hidden_layers': 2, 'dropout_rate': 0.45266228914260864, 'activation': 1}, {'hidden_size': 21, 'learning_rate': 0.07325429970026016, 'num_hidden_layers': 3, 'dropout_rate': 0.4275458753108978, 'activation': 2}, {'hidden_size': 39, 'learning_rate': 0.09714972537755967, 'num_hidden_layers': 1, 'dropout_rate': 0.4642423987388611, 'activation': 1}, None]\n",
            "Best validation losses: [0.0059849913232028484, 0.006929428782314062, 0.004834512248635292, 0.005206795409321785, 0.006397376302629709, 0.007975280284881592, 0.009558014571666718, inf]\n",
            "\n",
            "\n",
            "Epoch: 0 | MSE Train Loss: 0.0054429746232926846 | MSE Test Loss: 0.009711302816867828 \n",
            "Epoch: 10 | MSE Train Loss: 0.015305462293326855 | MSE Test Loss: 0.008109070360660553 \n",
            "Epoch: 20 | MSE Train Loss: 0.009938923642039299 | MSE Test Loss: 0.006402996368706226 \n",
            "Epoch: 30 | MSE Train Loss: 0.16949650645256042 | MSE Test Loss: 0.005029898136854172 \n",
            "Epoch: 40 | MSE Train Loss: 0.010749191977083683 | MSE Test Loss: 0.010621677152812481 \n",
            "Epoch: 50 | MSE Train Loss: 0.038483791053295135 | MSE Test Loss: 0.00943985115736723 \n",
            "Epoch: 60 | MSE Train Loss: 0.030978750437498093 | MSE Test Loss: 0.008075092919170856 \n",
            "Epoch: 70 | MSE Train Loss: 0.051300205290317535 | MSE Test Loss: 0.009662875905632973 \n",
            "Epoch: 80 | MSE Train Loss: 0.011877035722136497 | MSE Test Loss: 0.008021946996450424 \n",
            "Epoch: 90 | MSE Train Loss: 0.08042194694280624 | MSE Test Loss: 0.008374086581170559 \n",
            "Epoch: 100 | MSE Train Loss: 0.1267184615135193 | MSE Test Loss: 0.009371633641421795 \n",
            "Epoch: 110 | MSE Train Loss: 0.06052284687757492 | MSE Test Loss: 0.00980472844094038 \n",
            "Epoch: 120 | MSE Train Loss: 0.022275229915976524 | MSE Test Loss: 0.00991891324520111 \n",
            "Epoch: 130 | MSE Train Loss: 0.06203022599220276 | MSE Test Loss: 0.0065515791065990925 \n",
            "Epoch: 140 | MSE Train Loss: 0.016339369118213654 | MSE Test Loss: 0.007158711086958647 \n",
            "Epoch: 150 | MSE Train Loss: 0.01466398872435093 | MSE Test Loss: 0.006070768926292658 \n",
            "Epoch: 160 | MSE Train Loss: 0.10307621955871582 | MSE Test Loss: 0.005855918396264315 \n",
            "Epoch: 170 | MSE Train Loss: 0.010902654379606247 | MSE Test Loss: 0.008253988809883595 \n",
            "Epoch: 180 | MSE Train Loss: 0.0346023254096508 | MSE Test Loss: 0.010234585031867027 \n",
            "Epoch: 190 | MSE Train Loss: 0.05061348155140877 | MSE Test Loss: 0.01177292875945568 \n",
            "Epoch: 200 | MSE Train Loss: 0.021115625277161598 | MSE Test Loss: 0.009989992715418339 \n",
            "Epoch: 210 | MSE Train Loss: 0.07069773972034454 | MSE Test Loss: 0.007925515994429588 \n",
            "Epoch: 220 | MSE Train Loss: 0.2670387625694275 | MSE Test Loss: 0.012343849055469036 \n",
            "Epoch: 230 | MSE Train Loss: 0.11568395793437958 | MSE Test Loss: 0.0062882425263524055 \n",
            "Epoch: 240 | MSE Train Loss: 0.029981836676597595 | MSE Test Loss: 0.007360684685409069 \n",
            "Epoch: 250 | MSE Train Loss: 0.01882750168442726 | MSE Test Loss: 0.006542564369738102 \n",
            "Epoch: 260 | MSE Train Loss: 0.02073642797768116 | MSE Test Loss: 0.008998899720609188 \n",
            "Epoch: 270 | MSE Train Loss: 0.044340088963508606 | MSE Test Loss: 0.012051573023200035 \n",
            "Epoch: 280 | MSE Train Loss: 0.010625596158206463 | MSE Test Loss: 0.009450160898268223 \n",
            "Epoch: 290 | MSE Train Loss: 0.46930813789367676 | MSE Test Loss: 0.01480193343013525 \n",
            "Hyperparameters for best_model_8:\n",
            "{'hidden_size': 113, 'learning_rate': 0.06457395195961, 'num_hidden_layers': 3, 'dropout_rate': 0.21683502197265625, 'activation': 2}\n",
            "Validation Loss for best_model_8: 0.0054429746232926846\n",
            "Saving model to: models/best_model_8.pth\n",
            "Best hyperparameters: [{'hidden_size': 250, 'learning_rate': 0.07581899631023407, 'num_hidden_layers': 1, 'dropout_rate': 0.4119832515716553, 'activation': 2}, {'hidden_size': 155, 'learning_rate': 0.010746054828166964, 'num_hidden_layers': 2, 'dropout_rate': 0.0444769561290741, 'activation': 2}, {'hidden_size': 213, 'learning_rate': 0.09934136962890626, 'num_hidden_layers': 3, 'dropout_rate': 0.030684202909469604, 'activation': 1}, {'hidden_size': 64, 'learning_rate': 0.07174886053800583, 'num_hidden_layers': 3, 'dropout_rate': 0.10007864236831665, 'activation': 1}, {'hidden_size': 72, 'learning_rate': 0.04337182623147965, 'num_hidden_layers': 2, 'dropout_rate': 0.45266228914260864, 'activation': 1}, {'hidden_size': 21, 'learning_rate': 0.07325429970026016, 'num_hidden_layers': 3, 'dropout_rate': 0.4275458753108978, 'activation': 2}, {'hidden_size': 39, 'learning_rate': 0.09714972537755967, 'num_hidden_layers': 1, 'dropout_rate': 0.4642423987388611, 'activation': 1}, {'hidden_size': 113, 'learning_rate': 0.06457395195961, 'num_hidden_layers': 3, 'dropout_rate': 0.21683502197265625, 'activation': 2}]\n",
            "Best validation losses: [0.0059849913232028484, 0.006929428782314062, 0.004834512248635292, 0.005206795409321785, 0.006397376302629709, 0.007975280284881592, 0.009558014571666718, 0.0054429746232926846]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trial_info[0]['hyperparameters']['hidden_size']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C746K-T3nBT4",
        "outputId": "584e7a97-703b-46a0-db8c-c263ef70ad7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "250"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**arg:** <br/>\n",
        "num_trials: Number of random trials(Number of models I want to store) <br/>\n",
        "search_space: Dictionary defining the search space for hyperparameters <br/> output_size: Number of neurons in Linear layer <br/>\n",
        "device: Device <br/>\n",
        "train_loader: Data loader for training <br/>\n",
        "val_loader: Data loader for validation <br/>\n",
        "**MODEL_PATH = Path(\"models\")** <br/>\n",
        "**MODEL_PATH.mkdir(parents=True, exist_ok=True)** <br/>\n",
        "Creates a dir named \"models\" using the Path class <br/>\n",
        "If the dir already exists, it will not raise an error (exist_ok=True)<br/>\n",
        "If the parent directories don't exist, it will create them (parents=True)<br/>\n",
        "**best_losses = [float('inf')] * num_trials** <br/>\n",
        "**best_hyperparameters = [None] * num_trials** <br/>\n",
        "**best_models = [None] * num_trials** <br/>\n",
        "Initialize three lists (best_losses, best_hyperparameters, and best_models) to store information about the best models found during the random search.<br/>\n",
        "In each trial, this code randomly samples hyperparameters from the provided search space. The sampled hyperparameters include hidden size, learning rate, the number of hidden layers, dropout rate, and activation function.<br/>\n",
        "**hyperparameters = { 'hidden_size': torch.randint(search_space['hidden_size'][0], search_space['hidden_size'][1] + 1, (1,)).item(),\n",
        "    'learning_rate': torch.rand(1).item() * (search_space['learning_rate'][1] - search_space['learning_rate'][0]) + search_space['learning_rate'][0],\n",
        "    'num_hidden_layers': torch.randint(search_space['num_hidden_layers'][0], search_space['num_hidden_layers'][1] + 1, (1,)).item(),\n",
        "    'dropout_rate': torch.rand(1).item() * (search_space['dropout_rate'][1] - search_space['dropout_rate'][0]) + search_space['dropout_rate'][0],\n",
        "    'activation': torch.randint(0, len(search_space['activation']), (1,)).item()\n",
        "}** <br/>\n",
        "In each trial, this code randomly samples hyperparameters from the provided search space. The sampled hyperparameters include hidden size, learning rate, the number of hidden layers, dropout rate, and activation function. <br/>\n",
        "Random search  <br/>\n",
        "hidden_size(neurons), learning_rate, num_hidden_layers(Number of hidden layers), dropout_rate(dropout prob), activation:(Activation function)<br/>\n",
        "**model = Net(input_size=14,\n",
        "    hidden_size=hyperparameters['hidden_size'],\n",
        "    output_size=output_size,\n",
        "    num_hidden_layers=hyperparameters['num_hidden_layers'],\n",
        "    dropout_rate=hyperparameters['dropout_rate'],\n",
        "    activation=hyperparameters['activation'])** <br/>\n",
        "Creates a neural network model (Net) using the sampled hyperparameters.<br/>\n",
        "**Define loss function, optimizer**<br/>\n",
        "criterion = nn.MSELoss() <br/>\n",
        "optimizer = optim.SGD(model.parameters(), lr=hyperparameters['learning_rate']) <br/>\n",
        "**epoch_count, val_loss_values, test_loss_values = train_and_evaluate(model, criterion, optimizer, train_loader, val_loader, device)**<br/>\n",
        "Model is trained and evaluated on the training and validation datasets <br/>\n",
        "**val_loss = min(val_loss_values)** <br/>\n",
        "Minimum validation loss computed <br/>\n",
        "if val_loss < best_losses[trial - 1]: <br/>\n",
        "    best_losses[trial - 1] = val_loss <br/>\n",
        "    best_hyperparameters[trial - 1] = hyperparameters <br/>\n",
        "    best_models[trial - 1] = model <br/>\n",
        "    # Save the best model <br/>\n",
        "    MODEL_NAME = f\"best_model_{trial}.pth\" <br/>\n",
        "    MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME <br/>\n",
        "    torch.save(obj=model.state_dict(), f=MODEL_SAVE_PATH) <br/>\n",
        "    # Print hyperparameters and validation loss for the current trial <br/>\n",
        "    print(f\"Hyperparameters for best_model_{trial}:\") <br/>\n",
        "    print(hyperparameters) <br/>\n",
        "    print(f\"Validation Loss for best_model_{trial}: {val_loss}\") <br/>\n",
        "    print(f\"Saving model to: {MODEL_SAVE_PATH}\")  <br/>\n",
        "print(f'Best hyperparameters: {best_hyperparameters}') <br/>\n",
        "print(f'Best validation losses: {best_losses}') <br/>\n",
        "print(\"\\n\") <br/>\n",
        "If the current model has a lower validation loss than the best model so far, the best model is updated, and the model is saved to a file using torch.save. <br/>\n"
      ],
      "metadata": {
        "id": "lS90po4OZjYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model"
      ],
      "metadata": {
        "id": "0qBJnjfkX3F4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "MODEL_PATH = Path(\"models\")\n",
        "\n",
        "# Instantiate a new instance of our model for each trial\n",
        "loaded_models = []\n",
        "for trial in trial_info:\n",
        "    hyperparameters = trial['hyperparameters']\n",
        "    loaded_model = Net(hidden_size=hyperparameters['hidden_size'], num_hidden_layers=hyperparameters['num_hidden_layers'])\n",
        "    loaded_models.append(loaded_model)\n",
        "\n",
        "# Load the state_dict of our saved models\n",
        "for i, loaded_model in enumerate(loaded_models):\n",
        "    print(loaded_model.load_state_dict(torch.load(f=MODEL_PATH / f\"best_model_{i + 1}.pth\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVMPsTeroJxm",
        "outputId": "17e2e755-fb4a-4255-afa8-3560256ed960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<All keys matched successfully>\n",
            "<All keys matched successfully>\n",
            "<All keys matched successfully>\n",
            "<All keys matched successfully>\n",
            "<All keys matched successfully>\n",
            "<All keys matched successfully>\n",
            "<All keys matched successfully>\n",
            "<All keys matched successfully>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make Predictions"
      ],
      "metadata": {
        "id": "3JvWjhAgcz7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_predictions(targets, predictions, model_names):\n",
        "    \"\"\"\n",
        "    Plots predictions and target data for each model.\n",
        "    \"\"\"\n",
        "    # Set the number of columns and rows for the subplots\n",
        "    num_columns = 2\n",
        "    num_rows = 4\n",
        "    #X axis number\n",
        "    values_array = np.array([3295, 3296, 3297, 3298, 3299, 3300], dtype=np.float32).reshape((6, 1))\n",
        "    x_arrays = [values_array] * 8\n",
        "    #Target Data\n",
        "    val_df = targets\n",
        "    val = [np.array(val_df.iloc[:, -1]).reshape(-1, 1) for k in range(8)]\n",
        "\n",
        "    # Create subplots with the specified layout\n",
        "    fig, axs = plt.subplots(num_rows, num_columns, figsize=(15, 12))\n",
        "\n",
        "    # Flatten the axs array to simplify indexing\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    for i in range(len(model_names)):\n",
        "        model_name = model_names[i]\n",
        "        # Index to the appropriate subplot\n",
        "        ax = axs[i]\n",
        "        # Plot predictions in green\n",
        "        ax.scatter(x_arrays[i], predictions[i], c=\"g\", s=4, label=\"Predictions\")\n",
        "        # Plot target data in red\n",
        "        ax.scatter(x_arrays[i], val[i], c=\"r\", s=4, label=\"Target\")\n",
        "        # Customize subplot title\n",
        "        ax.set_title(f\"best_model_{i+1} Predictions vs. Target\")\n",
        "        # Set y-axis limits to 0.0 to 1.0\n",
        "        ax.set_ylim(0.0, 1.0)\n",
        "\n",
        "    # Add a single legend outside the subplots with custom labels\n",
        "    fig.legend(labels=['Green: Predictions', 'Red: Target'], loc='upper right', bbox_to_anchor=(1, 1))\n",
        "\n",
        "    # Adjust layout to prevent overlapping titles\n",
        "    plt.tight_layout()\n",
        "    # Show the plots\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6q2rAmmQNLVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Gather predictions for all models\n",
        "all_predictions = []\n",
        "with torch.no_grad():\n",
        "    for model in loaded_models:\n",
        "        model_predictions = []\n",
        "        for inputs, _ in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            # Move the model to the same device as the inputs\n",
        "            model = model.to(device)\n",
        "            model_pred = model(inputs)\n",
        "            model_predictions.append(model_pred.cpu().numpy())\n",
        "        all_predictions.append(np.concatenate(model_predictions))\n",
        "\n",
        "# Assuming you have val_df and model_names defined\n",
        "plot_predictions(test_dataset, all_predictions, loaded_models)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "qG6wRLlCWyeB",
        "outputId": "630e345d-dbdc-46ec-b470-d7a442aa0759"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-a2ae57cc625c>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_models\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mmodel_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'loaded_models' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def boxplot_predictions(targets, predictions, model_names):\n",
        "    \"\"\"\n",
        "    Plots predictions and target data for each model.\n",
        "    \"\"\"\n",
        "    # Set the number of columns and rows for the subplots\n",
        "    num_columns = 2\n",
        "    num_rows = 3\n",
        "    #X axis number\n",
        "    x_array = np.array([(3295+j) for j in range(6)], dtype=np.float32).reshape((6, 1))\n",
        "    #Target Data\n",
        "    val = np.array([targets.iloc[:, -1] ], dtype=np.float32).reshape((6, 1))\n",
        "    #Predictions\n",
        "    preds = []\n",
        "    for j in range(6):\n",
        "        val = np.array([predictions[i][j][-1] for i in range(8)], dtype=np.float32).reshape((8, 1))\n",
        "        preds.append(val)\n",
        "\n",
        "    # Create subplots with the specified layout\n",
        "    fig, axs = plt.subplots(num_rows, num_columns, figsize=(15, 12))\n",
        "\n",
        "    # Flatten the axs array to simplify indexing\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    for i in range(6):\n",
        "        # Index to the appropriate subplot\n",
        "        ax = axs[i]\n",
        "        # Plot predictions in green\n",
        "        ax.boxplot(preds[i], labels=['Predictions'])\n",
        "\n",
        "        # Get x-axis limits and calculate the midpoint\n",
        "        x_limits = ax.get_xlim()\n",
        "        x_position = np.mean(x_limits)\n",
        "\n",
        "        # Plot target data in red\n",
        "        ax.scatter(x_position,val[i][0],c=\"r\", s=4, label=\"Target\")\n",
        "\n",
        "        # Customize subplot title\n",
        "        ax.set_title(f\"Data_{i+3295} Predictions vs. Target\")\n",
        "        # Set y-axis limits to 0.0 to 1.0\n",
        "        ax.set_ylim(0.0, 1.0)\n",
        "\n",
        "    # Add a single legend outside the subplots with custom labels\n",
        "    fig.legend(labels=['BoxPlot: Predictions', 'Scatter: Target'], loc='upper right', bbox_to_anchor=(1, 1))\n",
        "    # Adjust layout to prevent overlapping titles\n",
        "    plt.tight_layout()\n",
        "    # Show the plots\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "7HsDK_5uKdEF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boxplot_predictions(test_dataset, all_predictions, loaded_models)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "m4s1MRT_Fusz",
        "outputId": "96c38997-2d41-4166-d2dd-fc872d32b15f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-be350b6d2442>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mboxplot_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaded_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_modelqualityloss(models, train_losses, test_losses, val_losses):\n",
        "    \"\"\"\n",
        "    Plots predictions and target data for each model.\n",
        "    \"\"\"\n",
        "    # Set the number of columns and rows for the subplots\n",
        "    num_columns = 3\n",
        "    num_rows = 1\n",
        "\n",
        "    # Create subplots with the specified layout\n",
        "    fig, axs = plt.subplots(num_rows, num_columns, figsize=(15, 12))\n",
        "\n",
        "    # Flatten the axs array to simplify indexing\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    for i in range(3):\n",
        "        # Index to the appropriate subplot\n",
        "        ax = axs[i]\n",
        "        # Plot train losses in green\n",
        "        ax.plot(train_losses[i], c=\"g\", s=4, label=\"Train Loss\")\n",
        "\n",
        "        # Plot test losses in red\n",
        "        ax.plot(test_losses[i], c=\"r\", s=4, label=\"Test Loss\")\n",
        "\n",
        "        # Plot val losses in red\n",
        "        ax.plot(val_losses[i], c=\"b\", s=4, label=\"Validation Loss\")\n",
        "\n",
        "        # Customize subplot title\n",
        "        ax.set_title(f\"Model_{i+1} MSE Loss\")\n",
        "        # Set y-axis limits to 0.0 to 1.0\n",
        "        ax.set_ylim(0.0, 1.0)\n",
        "\n",
        "    # Add a single legend outside the subplots with custom labels\n",
        "    fig.legend(labels=['Model Performance: Train MSE, Test MSE, Validation MSE'], loc='upper right', bbox_to_anchor=(1, 1))\n",
        "    # Adjust layout to prevent overlapping titles\n",
        "    plt.tight_layout()\n",
        "    # Show the plots\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "PS8h2dxAoCKD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}