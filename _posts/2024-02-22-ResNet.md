---
layout: post
title:  "ResNet"
---
# Overview
### What is ResNet?
ResNet is a super-powerful CNN that won the imageNet challenge back in 2015 <br/>
Is able to achieve incredible performance(Around 3.57 error) compared to human performance on classifying images <br/>
### Keypoint <br/>
(1)Able to stack many layers around 152 layers approximately within Resnet <br/>
(2)Overcame the problem known as vanishing gradient problem <br/>
**Limitations** <br/>
Before ResNets, whenever additional layers are stacked on top of basic networks such as LeNet or AlexNet, problem known as vanishing gradient tend to occur <br/>
One of the problems, when different layers are stacked on top of each other, the gradient becomes very small and the network performance becomes super poor <br/>
Network cannot be trained anymore if having a deep CNN network <br/>
**Enhancement** <br/>
Use skip connection <br/>
Skip connection? <br/>
![ResNet1](https://github.com/growingpenguin/growingpenguin.github.io/assets/110277903/5db255c1-963c-4975-9eea-ed350d092b3a) <br/>
Image Reference: https://gongdolhouse.tistory.com/entry/ResNet-skip-connection <br/>
Basically feed in the input to the next layer and perform skip connection <br/>
=> Feed in the input of the convolution layer and skip this layer and feed it into the next layer <br/>
By doing that, we can overcome the vanishing gradient issues and are able to stack hundreds of layers on top of each other <br/>
-In-Depth Description <br/>
(1) Layers to blocks <br/>
Group up layers of the network into blocks, and for each block, have data go and around <br/>
(2) Make Connections <br/>
Within each block, the layers will pass their data forward formally, but between blocks will have a new type of connection <br/>
The connection works by combining the input to the block to the output from the block, giving two different paths for the data to follow <br/>
One path going through the block and one that goes around the block <br/>
(3) Combining the data passed through the connections <br/>
Need some way of combining the output of the block with the input to the block <br/>
Want this to be a simple function that passes gradients along undisturbed <br/>
Take the tensor of inputs and outputs and add them either element wise or concatenate tensors <br/>
(4) Network Completed <br/>
Network built out of residual blocks <br/>
**Performance Comparison** <br/>
![ResNet2](https://github.com/growingpenguin/growingpenguin.github.io/assets/110277903/db44b6d1-8a3b-41c7-97a1-244aceec1d48) <br/>
Image Reference: https://towardsdatascience.com/understanding-and-coding-a-resnet-in-keras-446d7ff84d33 <br/>
Comparison between various networks <br/> 
AlexNet only has 8 layers, VGG network having around 19 layers <br/>
ResNet on the other hand was able to stack many layers on top of each other having a total of 152 layers <br/>
Showed massive improvement around 3.57% on ImageNet dataset <br/>
**Structure** <br/>
![ResNet4](https://github.com/growingpenguin/growingpenguin.github.io/assets/110277903/3460cf50-c313-4a03-a7ae-94a93308f9a1) <br/>
Having many layers like this is super-powerful, like having many human brains to classify images and recognize objects <br/>
There are bunch of convolution layers and here, we are feeding as is to the layer after skipping a few layer <br/>
By skipping repeatedly, we can overcome the vanishing gradient problem when training the convolutional neural network <br/> 
### Additional Features
-Used primarily as a backbone for computer vision tasks <br/>
![ResNet3](https://github.com/growingpenguin/growingpenguin.github.io/assets/110277903/f9aa100f-e4fb-4c33-9445-7c095f0c619c) <br/>
Footnote: <br/>
ImageNet: <br/>
Open-source repository of images contains 14+ million images and contains over 21,000 categories <br/>
Gradient: <br/>
Used to try to go back to the network and update its weights <br/> 




Reference: 
https://www.youtube.com/watch?v=nc7FzLiB_AY <br/>
https://towardsdatascience.com/understanding-and-coding-a-resnet-in-keras-446d7ff84d33 <br/>
https://www.researchgate.net/figure/The-structure-of-the-used-residual-neural-network-Shortcuts-for-residual-blocks-12-are_fig2_332927869 <br/>
https://www.youtube.com/watch?v=Q1JCrG1bJ-A <br/>
## Introduction

