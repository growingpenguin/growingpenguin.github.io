---
layout: post
title:  "Collect Large Dataset"
---
# 개요
원하는 Dataset이 모두 있을 때 <br/>
-Label된, 되지 않은 data가 모두 많은 경우 <br/>
-즉, Computing  자산이 많은 경우 
Transformer를 밑바닥부터 훈련 <br/>
-대용량 Dataset을 수집 및 관리 <br/>
-자신만의 Dataset을 위한 user defined Tokenizer 만들기 <br/>
-여러 GPU에서 대규모로 모델 훈련하기 <br/>
분산 Training 자체는 어렵지만 Accelerator 사용 가능 <br/>
<br/>
# 대규모 Dataset 수집하기 
기존 Language Model Fine tuning하는 이유 <br/>
-Corpus 크기 <br/>
-Pre-trained Model와 Corpus 분야 차이 <br/>
Tokenizer가 달라져야 하는 경우 생김 <br/>
Pre-trained Model에서는 빈도 수가 높았던 단어가 우리가 다루는 model에서는 빈도 수가 낮음 <br/>
ex. Midm vs. Llama <br/>
Midm: 한국어 많음 <-> Llama: 한국어 적음 <br/>
<br/>
대규모 Dataset 구축의 어려움 <br/>
1.(반)자동으로 취합된 Data의 위험 요소 <br/>
편향, 저품질 Data 포함 가능성 <br/>
-Bert(Book Corpus), T5(C4) Dataset <br/>
C4 <br/>
Corpus의 상당 부분 기계 번역 <br/>
불용어 필터링 과정에서 아프리카계 영어가 과도하게 삭제 <br/>
성적, 노골적 컨텐츠가 과도하게 삭제 <br/>
Book Corpus <br/>
저작권 위반 내용 상당 부분 포함 <br/>
로맨스 소설 장르 편향 <br/>
2.대규모 말뭉치 구축의 어려움: GPT vs. GPT-2 <br/>
GPT: <br/>
대부분 Book Corpus(Gpt 동시대 등장) <br/>
GPT-2: <br/>
웹페이지, 블로그, 레딧 <br/>
GPT size: 116.5M parameters <br/>
GPT2 size: 124.4M paramters <br/>
=> GPT, GPT-2 Corpus가 달라서 같은 prompt 입력해도 생성 결과가 달라짐 <br/>
<br/>
사용자 정의 Code Dataset 만들기 <br/>
Super Big Data Processing <br/>
-Github에서 직접 수집/가공 <br/>
-Google Big Query (구글 빅쿼리) 등 클라우드 고려 (SQL 형식의 query, 가공, 조회, 수정 가능) <br/>
-Hugging Face Dataset <br/>
transformersbook/codeparrat (70% 중복) <br/>
codeparrot/codeparrot-clean (중복 제거) <br/>
<br/>
-대용량 Dataset 다루기 <br/>
Code Dataset: 200GB <br/>
-HuggingFace datasets Package<br/>
Memory mapping, streaming 지원 <br/>
-Memory는 부족, 저장공간은 충분 <br/>
mmap(메모리 매핑) <br/>
:파일을 memory처럼 취급, Kernel을 거치지 않은 IO <br/>
Zero-copy: Kernel space ~ User space 간의 복사 제거 <br/>
Zero-overhead: IO작업시 Overhead 제거(최소화) <br/>
Page Fault 발생시 처리 O verhead 등이 발생하기는 함 <br/>
저장공간마저 부족 <br/>
Streaming <br/>
:IterableDataset 객체 생성: 'load_dataset(..., streaming=True) <br/>
즉시 return됨 <br/>
Cache file이 생성되지 않고, 큰 memory 필요치 않음 <br/>
Random Access 불가 <br/>
Shuffle()가능 <br/>
Local source, remote source 둘 다 지원됨 <br/>
=> 최후의 보루로 네트워크 속도만 보장된다면 HDD 용량 없다면 Datasetreaming 하면서 학습 가능 <br/>
<br/>
HuggingFace Hub에 Dataset 추가 <br/>
장점 <br/>
-Training Server에서 쉽게 다운 가능 <br/>
-Hub Dataset과 함께 사용 <br/>
-Community 공유 <br/>
Generate repo in my hub <br/>
-'huggingface-cli login' <br/>
-'huggingface-cli repo create -type datset -organization XXX codeparrot-train' <br/>
로컬 Clone/Data 추가/ Push (필요시 git lfs install 및 git lfs track *.json 등) <br/>
-git clone <br/>
-로컬에 Data 추가 <br/>
-git add <br/>
-git push <br/>
Hub 내 Repo에 README 카드 수정 <br/>
<br/>
# Tokenizer 구축하기
새로운 Language Model 훈련 <br/>
-기존 Tokenizer 사용시 단점 <br/>
T5 Tokenizer: 불용어 필터링 (sex 키워드 인식 불가) <br/>
CamememBERT: 프랑스어만 대상으로 학습 ('being'인식 불가) <BR/>
-새 Domain의 Encoding Sequence 길이가 증가하게 됨 <br/>
