---
layout: post
title:  "VIT(Vision Transformers)"
---
# VIT(Vision Transformers)
AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE <br/>

## Abstract
Limited use of Transformer architecture in computer vision tasks <br/>
-How is attention applied? <br/>
Applied in conjunction with or by replacing certain components of convolutional networks <br/>
-What is the overall theme of the thesis? <br/>
Reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks <br/>
-How does it show high performance? <br/>
Vision Transformer(ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train <br/>
Reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks <br/>

## Introduction
**NLP** <br/>
Use of Transformers(Self-Attention based structure) remain dominant <br/>
Pre-train on a large text corpus -> Finetune on a small task specific tasks <br/>
Transformer structure has computational efficiency and scalability, so it has become possible to train models of unprecedented size, with over 100B parameters <br/>
**Compter Vision** <br/>
Convolutional architectures remain dominant <br/>
Try combining CNN-like architectures with self-attention, some replacing the convolutions entirely <br/>
Problem: <br/>
Latter models, have not yet been scaled effectively on modern hardware accelerators due to the use of specialized attention patterns <br/>
Result: <br/>
In large-scale image recognition, classic ResNetlike architectures are still state of the art(Mahajanetal.,2018;Xieetal.,2020;Kolesnikovetal., 2020)
**VIT Experiment Process** <br/>
How? <br/>
Apply a standard Transformer directly to images <br/>
(1)Split an image into patches <br/>
:Dividing the input image into smaller, fixed-size pieces, known as patches <br/> 
-Patches are treated the same way as tokens <br/>
-Similar to how you might divide a large picture into a grid of smaller squares <br/> 
-Size of these patches is predetermined and is a critical parameter that can influence the model's performance <br/> 
-Example: Have a 224x224 pixel image and you decide to split it into patches of 16x16 pixels, you'll end up with 196 (14x14) patches <br/> 
(2)Provide the sequence of linear embeddings of these patches as an input to a Transformer <br/> 
(2)-1 Linear embeddings <br/>  
Each patch is then flattened(converted from a 2D array of pixels to a 1D array) and passed through a linear layer (also known as a fully connected layer) <br/>
-Purpose of this embedding is to transform the raw pixel values into a form that captures more meaningful information about the content of the patch, making it more suitable for processing by deep learning models <br/>
These embeddings can capture various features of the image, like edges, textures, or more complex patterns, depending on the training <br/>
(2)-2 Sequence <br/>
Embeddings of the patches are treated as a sequence <br/>
Key aspect because Transformers, originally developed for NLP tasks, are designed to process sequential data <br/>
(2)-3 Input to a Transformer <br/>
Sequence of embeddings is fed into a Transformer model <br/>
Transformer uses self-attention mechanisms to weigh the importance of different patches relative to each other for the task at hand (e.g., image classification) <br/>
Can learn to focus more on the embeddings of patches that contain critical information (like the object of interest) and less on those that are less relevant (like the background) <br/>





This layer projects the flattened patch into a higher-dimensional space, creating what is known as an embedding <br/>
The purpose of this embedding is to transform the raw pixel values into a form that captures more meaningful information about the content of the patch, making it more suitable for processing by deep learning models <br/> 
These embeddings can capture various features of the image, like edges, textures, or more complex patterns, depending on the training <br/>

Each patch is then flattened (converted from a 2D array of pixels to a 1D array) and passed thr



Attention mechanisms can be added to CNNs to enhance their ability to focus on the most informative parts of an image, improving the model's performance on tasks like image classification, object detection, and more <br/>
-Used to replace certain components of convolutional networks: <br/>
Attention mechanisms can also be used as a substitute for specific parts of a CNN <br/>
Example: <br/>
Instead of using a traditional convolutional layer, a model might use an attention-based layer designed to perform a similar function but with the added benefit of the attention mechanism's focused processing <br/>


Applications to computervision remain limited <br/>
In vision,attentioniseitherappliedinconjunctionwithconvolutionalnetworks,or usedtoreplacecertaincomponentsofconvolutionalnetworkswhilekeepingtheir overallstructureinplace.Weshowthat thisrelianceonCNNsisnotnecessary andapuretransformerapplieddirectlytosequencesofimagepatchescanperform verywellonimageclassificationtasks. Whenpre-trainedonlargeamountsof dataandtransferredtomultiplemid-sizedorsmallimagerecognitionbenchmarks (ImageNet,CIFAR-100,VTAB,etc.),VisionTransformer(ViT)attainsexcellent resultscomparedtostate-of-the-artconvolutionalnetworkswhilerequiringsubstantiallyfewercomputationalresourcestotrain.1

Attention mechanisms: <br/>
Allow models to focus on specific parts of the input data that are most relevant for the task at hand <br/>
