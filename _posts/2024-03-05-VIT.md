---
layout: post
title:  "VIT(Vision Transformers)"
---
# VIT(Vision Transformers)
AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE <br/>

## ABSTRACT
Limited use of Transformer architecture in computer vision tasks <br/>
-How is attention applied? <br/>
Applied in conjunction with or by replacing certain components of convolutional networks <br/>
-Why ? <br/>
Reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks <br/>




Attention mechanisms can be added to CNNs to enhance their ability to focus on the most informative parts of an image, improving the model's performance on tasks like image classification, object detection, and more <br/>
-Used to replace certain components of convolutional networks: <br/>
Attention mechanisms can also be used as a substitute for specific parts of a CNN <br/>
Example: <br/>
Instead of using a traditional convolutional layer, a model might use an attention-based layer designed to perform a similar function but with the added benefit of the attention mechanism's focused processing <br/>


Applications to computervision remain limited <br/>
In vision,attentioniseitherappliedinconjunctionwithconvolutionalnetworks,or usedtoreplacecertaincomponentsofconvolutionalnetworkswhilekeepingtheir overallstructureinplace.Weshowthat thisrelianceonCNNsisnotnecessary andapuretransformerapplieddirectlytosequencesofimagepatchescanperform verywellonimageclassificationtasks. Whenpre-trainedonlargeamountsof dataandtransferredtomultiplemid-sizedorsmallimagerecognitionbenchmarks (ImageNet,CIFAR-100,VTAB,etc.),VisionTransformer(ViT)attainsexcellent resultscomparedtostate-of-the-artconvolutionalnetworkswhilerequiringsubstantiallyfewercomputationalresourcestotrain.1

Attention mechanisms: <br/>
Allow models to focus on specific parts of the input data that are most relevant for the task at hand <br/>
