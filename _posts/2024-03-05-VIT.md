---
layout: post
title:  "VIT(Vision Transformers)"
---

# VIT(Vision Transformers)
AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE <br/>

## Abstract
-Limited application of Transformer architecture to computer vision tasks compared to NLP <br/>
Traditionally, when utilized for computer vision, attention mechanisms were integrated with or served as replacements for specific parts of convolutional neural networks (CNNs) <br/>
-Objective of the Thesis <br/>
Challenges the necessity of coupling Transformers with CNNs for image tasks <br/>
Purely Transformer-based approach, which processes sequences of image patches directly, can effectively handle image classification tasks without the need for convolutional operations <br/>
-VIT stages of training <br/>
(1)Pre-trained on large datasets <br/>
(2)Capabilities extended to various medium and small-sized image recognition benchmarks through transfer learning <br/>
-Performance of VIT <br/>
Performance of ViT is noteworthy, achieving excellent results that often surpass those of advanced CNNs while being more efficient in terms of computational resources required for training <br/>

## Introduction
**NLP** <br/>
-In the domain of NLP, the utilization of Transformers, which are based on self-attention mechanisms, remains predominant <br/>
-Process <br/>
Pre-trained on vast corpora of text, which is then followed by fine-tuning on smaller, task-specific datasets <br/>
-Appeal of Transformers <br/>
Structural advantages offer computational efficiency and scalability <br/>
Has even enabled the training of extraordinarily large models, some with over 100 billion parameters, marking a significant advancement in the field's capabilities <br/>

**Compter Vision** <br/>
-Convolutional architectures remain dominant <br/>
-Approaches to use Transformer Architecture <br/>
(1)Try combining CNN-like architectures with self-attention <br/>
(2)Replacing the convolutions entirely <br/>
Problem: <br/>
Have not yet been scaled(like GPUs and TPUs) effectively on modern hardware accelerators, which are crucial for performing large-scale computations efficiently due to the use of specialized attention patterns. Unlike convolutions, which have been highly optimized over the years for parallel processing on such hardware, the attention mechanisms often involve more complex, irregular computations that do not map as efficiently onto these architectures <br/>

**VIT Experiment Process** <br/>
Streamlined Explanation of the process <br/>
Apply a standard Transformer directly to images with fewest modifications <br/>
(1)Image Patching <br/>
:Divide the input image into smaller, fixed-size segments called patches, akin to cutting a large picture into a grid of smaller squares <br/> 
-Patches are analogous to tokens in NLP, serving as the basic units for processing. <br/>
-Similar to how you might divide a large picture into a grid of smaller squares <br/> 
-Size of these patches is a crucial parameter, influencing the model's performance <br/> 
-Example: splitting a 224x224 pixel image into 16x16 pixel patches results in 196 (14x14) patches <br/> 
(2)Patch Embedding and Processing: <br/>
:Provide the sequence of linear embeddings of these patches as an input to a Transformer <br/> 
(2)-1 Linear embeddings <br/>  
Each patch is then flattened(converted from a 2D array of pixels to a 1D array) and passed through a linear layer (also known as a fully connected layer) <br/>
This step transforms the raw pixel values into more meaningful representations, potentially capturing features like edges, textures, or more complex patterns, enhancing their suitability for deep learning models <br/>
(2)-2 Sequence Formation <br/>
The embeddings are treated as a sequence <br/> 
Key aspect because Transformers, originally developed for NLP tasks, are designed to process sequential data <br/>
(2)-3 Input to a Transformer <br/>
Sequence of embeddings is fed into a Transformer model <br/>
Transformer uses self-attention mechanisms to weigh the importance of different patches relative to each other for the task at hand (e.g., image classification) <br/>
Can learn to focus more on the embeddings of patches that contain critical information (like the object of interest) and less on those that are less relevant (like the background) <br/>
(2)-4 Model Training <br/>
Model is trained on image classification tasks in a supervised manner, learning to accurately identify and categorize images based on the training data provided <br/>

**VIT trained in mid-sized datasets** <br/>
When Vision Transformer (ViT) models are trained on mid-sized datasets like ImageNet without the application of strong regularization techniques. Under these conditions, the models tend to achieve accuracies that are slightly lower, by a few percentage points, compared to ResNet models of comparable size <br/>
Transformers lack some of the inductive biases inherent to CNNs <br/>
-translation equivariance <br/>
If you shift the input (e.g., an image), the output shifts in the same way <br/>
CNNs naturally possess this trait due to the way convolutions are applied across an image, ensuring that the network's perception of objects is consistent regardless of their position in the image <br/>
-locality <br/>
CNNs process images in a way that gives priority to local features before integrating them into a more global understanding <br/>
Achieved through the use of small filters that operate on local patches of an image, capturing details like edges and textures before later layers aggregate this information to recognize more complex patterns <br/>
-Because Transformers do not inherently have these biases, they may not generalize as effectively when trained on smaller amounts of data <br/> CNNs, with their built-in biases towards translation equivariance and locality, might require less data to learn comparable generalizations <br/>

**VIT trained in larger datasets(14M-300M images)** <br/>
Large scale training trumps inductive bias <br/>
-VisionTransformer(ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints <br/>
-Example: <br/>
Pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks <br/> 
Best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks <br/> 

Footnote: <br/>
Attention mechanisms: <br/>
Allow models to focus on specific parts of the input data that are most relevant for the task at hand <br/>
