---
layout: post
title:  "VIT(Vision Transformers)"
---

# VIT(Vision Transformers)
AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE <br/>

## Abstract
-Limited application of Transformer architecture to computer vision tasks compared to NLP <br/>
Traditionally, when utilized for computer vision, attention mechanisms were integrated with or served as replacements for specific parts of convolutional neural networks (CNNs) <br/>
-Objective of the Thesis <br/>
Challenges the necessity of coupling Transformers with CNNs for image tasks <br/>
Purely Transformer-based approach, which processes sequences of image patches directly, can effectively handle image classification tasks without the need for convolutional operations <br/>
-VIT stages of training <br/>
(1)Pre-trained on large datasets <br/>
(2)Capabilities extended to various medium and small-sized image recognition benchmarks through transfer learning <br/>
-Performance of VIT <br/>
Performance of ViT is noteworthy, achieving excellent results that often surpass those of advanced CNNs while being more efficient in terms of computational resources required for training <br/>

## Introduction
**NLP** <br/>
-In the domain of NLP, the utilization of Transformers, which are based on self-attention mechanisms, remains predominant <br/>
-Process <br/>
Pre-trained on vast corpora of text, which is then followed by fine-tuning on smaller, task-specific datasets <br/>
-Appeal of Transformers <br/>
Structural advantages offer computational efficiency and scalability <br/>
Has even enabled the training of extraordinarily large models, some with over 100 billion parameters, marking a significant advancement in the field's capabilities <br/>

**Compter Vision** <br/>
-Convolutional architectures remain dominant <br/>
-Approaches to use Transformer Architecture <br/>
(1)Try combining CNN-like architectures with self-attention <br/>
(2)Replacing the convolutions entirely <br/>
Problem: <br/>
Have not yet been scaled(like GPUs and TPUs) effectively on modern hardware accelerators, which are crucial for performing large-scale computations efficiently due to the use of specialized attention patterns. Unlike convolutions, which have been highly optimized over the years for parallel processing on such hardware, the attention mechanisms often involve more complex, irregular computations that do not map as efficiently onto these architectures <br/>

**VIT Experiment Process** <br/>
Streamlined Explanation of the process <br/>
Apply a standard Transformer directly to images with fewest modifications <br/>
(1)Image Patching <br/>
:Divide the input image into smaller, fixed-size segments called patches, akin to cutting a large picture into a grid of smaller squares <br/> 
-Patches are analogous to tokens in NLP, serving as the basic units for processing. <br/>
-Similar to how you might divide a large picture into a grid of smaller squares <br/> 
-Size of these patches is a crucial parameter, influencing the model's performance <br/> 
-Example: splitting a 224x224 pixel image into 16x16 pixel patches results in 196 (14x14) patches <br/> 
(2)Patch Embedding and Processing: <br/>
:Provide the sequence of linear embeddings of these patches as an input to a Transformer <br/> 
(2)-1 Linear embeddings <br/>  
Each patch is then flattened(converted from a 2D array of pixels to a 1D array) and passed through a linear layer (also known as a fully connected layer) <br/>
This step transforms the raw pixel values into more meaningful representations, potentially capturing features like edges, textures, or more complex patterns, enhancing their suitability for deep learning models <br/>
(2)-2 Sequence Formation <br/>
The embeddings are treated as a sequence <br/> 
Key aspect because Transformers, originally developed for NLP tasks, are designed to process sequential data <br/>
(2)-3 Input to a Transformer <br/>
Sequence of embeddings is fed into a Transformer model <br/>
Transformer uses self-attention mechanisms to weigh the importance of different patches relative to each other for the task at hand (e.g., image classification) <br/>
Can learn to focus more on the embeddings of patches that contain critical information (like the object of interest) and less on those that are less relevant (like the background) <br/>
(2)-4 Model Training <br/>
Model is trained on image classification tasks in a supervised manner, learning to accurately identify and categorize images based on the training data provided <br/>


When trained on mid-sized datasets such as ImageNet without strong regularization, these models yield modest accuracies of a few percentage points below ResNets of comparable size. <br/>
This seemingly discouraging out come maybe expected: Transformers lack some of the inductive biases




This layer projects the flattened patch into a higher-dimensional space, creating what is known as an embedding <br/>
The purpose of this embedding is to transform the raw pixel values into a form that captures more meaningful information about the content of the patch, making it more suitable for processing by deep learning models <br/> 
These embeddings can capture various features of the image, like edges, textures, or more complex patterns, depending on the training <br/>

Each patch is then flattened (converted from a 2D array of pixels to a 1D array) and passed thr



Attention mechanisms can be added to CNNs to enhance their ability to focus on the most informative parts of an image, improving the model's performance on tasks like image classification, object detection, and more <br/>
-Used to replace certain components of convolutional networks: <br/>
Attention mechanisms can also be used as a substitute for specific parts of a CNN <br/>
Example: <br/>
Instead of using a traditional convolutional layer, a model might use an attention-based layer designed to perform a similar function but with the added benefit of the attention mechanism's focused processing <br/>


Applications to computervision remain limited <br/>
In vision,attentioniseitherappliedinconjunctionwithconvolutionalnetworks,or usedtoreplacecertaincomponentsofconvolutionalnetworkswhilekeepingtheir overallstructureinplace.Weshowthat thisrelianceonCNNsisnotnecessary andapuretransformerapplieddirectlytosequencesofimagepatchescanperform verywellonimageclassificationtasks. Whenpre-trainedonlargeamountsof dataandtransferredtomultiplemid-sizedorsmallimagerecognitionbenchmarks (ImageNet,CIFAR-100,VTAB,etc.),VisionTransformer(ViT)attainsexcellent resultscomparedtostate-of-the-artconvolutionalnetworkswhilerequiringsubstantiallyfewercomputationalresourcestotrain.1

Attention mechanisms: <br/>
Allow models to focus on specific parts of the input data that are most relevant for the task at hand <br/>
