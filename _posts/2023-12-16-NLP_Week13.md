---
layout: post
title:  "Understanding ChatGPT"
---
# ChatGPT: High Level View 
Step1: Collect Demonstration Data, and train a supervised policy<br/>
High alignment in Language Model <br/>
-Demonstration Data: <br/>
이런 prompt에는 이런 거 담아/생성하지 마 <br/>
-(1) A prompt is sampled from our prompt dataset <br/>
ex. Explain the moon landing to a 6 year old <br/>
(2) A labeler demonstrates the desired output behavior <br/>
ex. Some people went to the moon ~ <br/>
(3) This data is used to fine-tune GPT-3 with supervised learning <br/>
Step2:Collect comparison data, and train a reward model <br/>
Reinforcement Learning from Human Feedback <br/>
-(1) A prompt and several model outpus are sampled. <br/>
ex. Explain the moon landing to a 6 year old <br/>
Outputs from different models <br/>
A: Explain Gravity.. , B: Explain War..., C: Moon is a natural satellite of ! , D: People went to the moon <br/>
(2) Labeler ranks the outputs best to worst <br/>
(3) This data is used to train the reward model <br/>
Step3: Optimize a policy against the reward model using rl <br/>
-(1) A new prompt is sampled from the dataset <br/> 
ex. Write a story about frogs 
(2) The policy generates an output (PPO Trainer) <br/>
ex. Once upon a time~ <br/>
(3) The reward model calculates a reward for the output <br/>
(4) The reward is used to update the policy using PPO <br/>
<br/>
# ChatGPT Train Step1 
Supervised Fine Tuning (SFT:지도 학습 기반 Fine Tuning) <br/>
-Dataset: 약 13K <br/>
13000개의 dataset (prompt, answer) sample한 점이 놀라운 점 <br/>
GPT API 서비스에 수집된 Command Prompt(일부는 Labeler가 추가) <br/>
이에 대한 정답은 Labeler가 작성 <br/>
-Foundation model(GPT-3)를 Dataset에 대해 FineTuning <br/>
Result <br/>
-Foundation model이 명령-응답형 생성을 위한 워밍업이 갖춰진 상태 <br/>
-주관적인 만족스러움에 대한 지표가 충분히 반영되지 않은 상태 <br/>
<br/>
# ChatGPT Train Step2
만족스러운 지표 학습(RM: A reward model by supervised learning) <br/>
-Dataset: 약 33K <br/>
GPT API service 및 labeler 작성 prompt <br/>
Model은 이에 대한 다수의 응답 목록 생성 <br/>
-Labeling <br/>
목록에서 응답 둘을 쌍으로 Labeler에게 Display <br/>
Labeler는 둘 중 만족스러운 응답을 승자로 Tagging <br/>
RLHF(Human Feedback) <br/>
-Ranking Model(Reward Model) <br/>
일대일 게임에 대한 결과(승자 Tagging)을 모으면 General한 Ranking 함수 학습 가능 <br/>
6B size model을 Supervised Learning <br/>
Result <br/>
-이후 RL Reward Model로 삼음 <br/>
-응답 지표(Reward)를 제시함으로써, RL을 통한 Finetuning이 가능하게 해줌 <br/>

# ChatGPT Train Step3 
만족스러운 지표를 추종하도록 언어모델 Fine tuning (LLM finetuned by RL:PPO) <br/>
-인간의 개입없이 31K Prompt training <br/>
-State(상태): <br/>
GPT API Service에서 Sampling한 Prompt <br/>
-Action(액션): <br/>
언어 모델은 응답 생성 <br/>
-Reward(보상): <br/>
Step2에서 학습한 Reward Model로부터 응답에 대한 보상 수신 <br/>
-Training(학습): <br/>
응답에 대한 보상으로부터 Reinforce)를 반영하여 Language Model Parameter 수정 <br/>
Result <br/>
설계한 지표를 추종하도록 Finetuning된 ChatGPT <br/>

# RLHF 요약 
강화학습 목표식 <br/>
KL항 <br/>
-Kullback-Leibler divergence <br/>
-두 분포가 다를 수록 큰 값 <br/>
Reward항 <br/>
-Reward Model 평가값 <br/>
의미 <br/>
-보상을 키우는 쪽으로 <br/>
-단, SFT와 너무 달라지는 것을 지양 <br/>
