---
layout: post
title:  "Understanding ChatGPT"
---
# ChatGPT: High Level View 
Step1: Collect Demonstration Data, and train a supervised policy<br/>
High alignment in Language Model <br/>
-Demonstration Data: <br/>
이런 prompt에는 이런 거 담아/생성하지 마 <br/>
-(1) A prompt is sampled from our prompt dataset <br/>
ex. Explain the moon landing to a 6 year old <br/>
(2) A labeler demonstrates the desired output behavior <br/>
ex. Some people went to the moon ~ <br/>
(3) This data is used to fine-tune GPT-3 with supervised learning <br/>
Step2:Collect comparison data, and train a reward model <br/>
Reinforcement Learning from Human Feedback <br/>
-(1) A prompt and several model outpus are sampled. <br/>
ex. Explain the moon landing to a 6 year old <br/>
Outputs from different models <br/>
A: Explain Gravity.. , B: Explain War..., C: Moon is a natural satellite of ! , D: People went to the moon <br/>
(2) Labeler ranks the outputs best to worst <br/>
(3) This data is used to train the reward model <br/>
Step3: Optimize a policy against the reward model using rl <br/>
-(1) A new prompt is sampled from the dataset <br/> 
ex. Write a story about frogs 
(2) The policy generates an output (PPO Trainer) <br/>
ex. Once upon a time~ <br/>
(3) The reward model calculates a reward for the output <br/>
(4) The reward is used to update the policy using PPO <br/>
<br/>
# ChatGPT Train Step1 
Supervised Fine Tuning (SFT:지도 학습 기반 Fine Tuning) <br/>
-Dataset: 약 13K <br/>
13000개의 dataset (prompt, answer) sample한 점이 놀라운 점 <br/>
GPT API 서비스에 수집된 Command Prompt(일부는 Labeler가 추가) <br/>
이에 대한 정답은 Labeler가 작성 <br/>
-Foundation model(GPT-3)를 Dataset에 대해 FineTuning <br/>
Result <br/>
-Foundation model이 명령-응답형 생성을 위한 워밍업이 갖춰진 상태 <br/>
-주관적인 만족스러움에 대한 지표가 충분히 반영되지 않은 상태 <br/>
<br/>
# ChatGPT Train Step2
만족스러운 지표 학습(RM: A reward model by supervised learning) <br/>
-Dataset: 약 33K <br/>
GPT API service 및 labeler 작성 prompt <br/>
Model은 이에 대한 다수의 응답 목록 생성 <br/>
-Labeling <br/>
목록에서 응답 둘을 쌍으로 Labeler에게 Display <br/>
Labeler는 둘 중 만족스러운 응답을 승자로 Tagging <br/>
RLHF(Human Feedback) <br/>
-Ranking Model(Reward Model) <br/>
일대일 게임에 대한 결과(승자 Tagging)을 모으면 General한 Ranking 함수 학습 가능 <br/>
6B size model을 Supervised Learning <br/>
Result <br/>
-이후 RL Reward Model로 삼음 <br/>
-응답 지표(Reward)를 제시함으로써, RL을 통한 Finetuning이 가능하게 해줌 <br/>

# ChatGPT Train Step3 
만족스러운 지표를 추종하도록 언어모델 Fine tuning (LLM finetuned by RL:PPO) <br/>
-인간의 개입없이 31K Prompt training <br/>
-State(상태): <br/>
GPT API Service에서 Sampling한 Prompt <br/>
-Action(액션): <br/>
언어 모델은 응답 생성 <br/>
-Reward(보상): <br/>
Step2에서 학습한 Reward Model로부터 응답에 대한 보상 수신 <br/>
-Training(학습): <br/>
응답에 대한 보상으로부터 Reinforce)를 반영하여 Language Model Parameter 수정 <br/>
Result <br/>
설계한 지표를 추종하도록 Finetuning된 ChatGPT <br/>

# RLHF 요약 
강화학습 목표식 <br/>
KL항 <br/>
-Kullback-Leibler divergence <br/>
-두 분포가 다를 수록 큰 값 <br/>
Reward항 <br/>
-Reward Model 평가값 <br/>
의미 <br/>
-보상을 키우는 쪽으로 <br/>
=> Base Model, Reward Model 분포 너무 다르지 않게, Reward는 극대화되도록 Training <br/>
![NLP_RLHF_Week13](https://github.com/growingpenguin/growingpenguin.github.io/assets/110277903/f7453161-aff4-4eb2-ae90-413e98b57c74)
<br/>
-단, SFT와 너무 달라지는 것을 지양 <br/>
<br/>
# ChatGPT 요약 
-Foundation Model GPT-3는 방대한 언어 지식이 자기 회귀 모델 자체에 코딩이 됨 <br/>
-채팅 형식의 명령 추종형 생성에 특화 <br/>
-RLHF: 명령에 대한 적절한 응답에 대한 Ranking modeling을 Human Feedback을 기반으로 학습 <br/>
-세상을 대격변 시키는 동력으로 작용 중 <br/>
<br/>
# 최근 동향 
-유사 ChatGPT 구축론 <br/>
-거대 언어모형 학습 최적화 <br/>
-거대 언어 모형 응용 최적화 관점 <br/>
-Google Bard (PaLM2) <br/>
<br/>
## 최근 동향: Self-Instruct
Content: <br/>
-Seed Task Set <br/>
저자들이 생성한 175개의 명령(instruction) 예제들 (명령어 당 Instance 하나)<br/>
-예제 일부를 조합하여 Prompt를 작성하고 이로부터 후속 예제 (instance)를 API(GPT-3)로부터 확보 <br/>
총 52K 명령에 대한 82K의 예제 Dataset 구축 <br/>
그 과정에서 Filtering 등을 통해 고품질의 예제만 남김 <br/>
-확보된 82K 명령-예제 Dataset을 활용하여 Vanilla GPT-3 Model을 Finetuning <br/>
Result <br/>
ChatGPT에 준하는 성능 확보 <br/>
의미 <br/>
High level Labeling 작업 (Supervised Learning, RLHF)없이 효과적인 ChatGPT 스타일의 Model 구축 대안 방법 <br/>
<br/>
## 최근 동향: LLama
-Scaling 법칙이 바뀜 <br/>
동일 Computing 자원으로는 Model Size를 키우는게 Dataset을 키우는 것보다 더 효과적이다 <br/>
작은 Model에서 더 많은 token을 학습시키는 것이 optimized 접근이다. <br/>
-LLaMA 의미 <br/>
공개 Model로서 민간에서 Fine tuning 또는 Inference용으로 접근 가능한 size <br/>
13B model이 GPT-3 성능 상회 <br/>
-학습은 여전히 많은 자원 필요 <br/>
<br/>
## 최근 동향: Stanford Alpaca
Self-Instruct 형식으로 명령 Dataset 확보 후 LLaMa 7B(13B)를 Fine Tuning  <br/>
Self-Instruct와 변경점 <br/>
-명령 Dataset 구축시 ChatGPT 사용 <br/>
Prompt Template + Seed 명령 Set으로 Prompt 생성 <br/>
구축 과정 간소화 <br/>
52K Dataset 생성: 약 $500 비용으로 사람에 비해 훨씬 저렴함 <br/>
LLAMA 7B(13B)에 대해 Fine Tuning <br/>
3시간: 8대의 A100 <br/>
영어에 대한 만족스러운 성과 (ChatGPT와 유사) <br/>
의미 <br/>
-RLHF를 통한 ChatGPT는 Nice한 방법이지만 Human Labeler 비용 소모 <br/>
-강력한 성능으로 학습된 ChatGPT로부터 정답 Set을 구성해서 Finetuning하는 전략 <br/>
-아쉬운 점: 한국어 Corpus에는 취약 <br/>
<br/>
# ChatGPT Plugin 
A tool designed to extend the capabilities of the ChatGPT language model. <br/>
These plugins are essentially add-ons that allow ChatGPT to access external resources, perform specific tasks, or improve its overall performzance. <br/>
Process of ChatGPT Plugin <br/>
1. User Input: You start by providing ChatGPT with a prompt or question. <br/>
2. Plugin Decision: ChatGPT analyzes the prompt and identifies keywords or phrases that might trigger a plugin. This analysis is done using its internal natural language processing (NLP) capabilities.<br/>
3. Plugin Activation: If a relevant plugin is identified, it's activated by ChatGPT. This involves sending information about the prompt and any relevant context to the plugin's API.<br/>
4. Plugin Execution: The activated plugin then performs its specific task.
This could involve:<br/>
Accessing external data: The plugin might connect to real-time data sources, knowledge bases, or APIs to gather relevant information.<br/>
Performing computations: Some plugins might run calculations, simulations, or other processes based on the received data.<br/>
Generating content: Certain plugins might generate creative text formats like poems, scripts, or code based on the prompt.<br/>
5. Plugin Results: Once the plugin finishes its task, it sends the results back to ChatGPT.<br/>
6. NLP Interpretation: Finally, ChatGPT uses its NLP capabilities again to interpret the plugin's results and integrate them into its response. This means understanding the context and format of the results and blending them seamlessly with its own understanding of the prompt.<br/>
7. Response to User: ChatGPT then presents the final response to you, incorporating both its own understanding and the information provided by the plugin.<br/>
<br/>
# Generative Model 활용 Framework 
LangChain: <br/>
-언어 모형 활용 응용 프로그램 개발 Framework <br/>
GPT-4와 External Source data 결합 <br/>
벡터 DB 등 Custom 문서 저장 및 조회 방법과 결합 <br/>
추론은 GPT-4 활용 <br/>
-문제 해결에 필요한 개발 요소 결합<br/>
AutoGPT: <br/>
문제 해결을 위한 prompt 생성 <br/>
External Source 조회 및 저장 <br/>
Prompt 확장 <br/>
LangChain으로 구현 가능 <br/>
<br/>
# PaLM 
:Scaling language modeling with pathways <br/>
-Decoder 기반, parameters 540B, 학습 token 수: 780B <br/>
컴퓨틴 최적화된 확장: Data, Model Scaling 1:1 <br/>
Dataset/문제 혼합: 다국어, 병렬 다국어, Programming, 수학 등 전문분야, Toxity에 대한 개선 <br/>
평가: 텍스트 이해 / 생성 능력확장, 영어 문서 비율 낮아졌으나 성능 높아짐 <br/>
특이점: 영어 이외 영역에 일본어/한국어 targeting <br/>
