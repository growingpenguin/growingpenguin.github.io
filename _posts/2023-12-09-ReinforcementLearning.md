---
layout: post
title:  "Reinforcement Learning"
---
# Week11
TD(0) <br/>
Original 1-step TD algorithm, which updates the state-value function based on the immediate reward and the estimated value of the next state.
**n-step TD** <br/>
Incorporates additional information from the future compared to the standard 1-step TD. It uses the return obtained over n steps, instead of the immediate reward, to update the state-value function.
**TD(Lambda)** <br/>
Lambda rewardëŠ” ëª¨ë“  n-stepê¹Œì§€ì˜ ê°€ì¤‘í‰ê· ëœ ë³´ìƒ. <br/>
-ëŒë‹¤ì˜ ì´í•©ì´ 1ì´ ë˜ë„ë¡ í•˜ê¸° ìœ„í•´ì„œ (1 - ëŒë‹¤) ê³„ìˆ˜ë¡œÂ ë…¸ë©€ë¼ì´ì¦ˆë¥¼ í•˜ì—¬ 0ë¶€í„° 1ê¹Œì§€ì˜ ê°’ì„ ê°–ë„ë¡ í•œë‹¤.<br/> 
-Higher the value of Î», the slower the decay, leading to a stronger emphasis on long-term rewards. Conversely, a lower Î» places more weight on the immediate reward, similar to 1-step TD.<br/> 
-ëŒë‹¤ëŠ” n stepì´ ì»¤ì§ˆìˆ˜ë¡ ë³´ìƒì— ëŒ€í•œ ê°’ì„ ê°ì†Œì‹œí‚¤ê²Œ ì‘ìš©í•œë‹¤<br/>
-Benefits <br/>
Stabilizes learning: By diminishing the impact of distant rewards, the decaying function helps to stabilize the learning process and avoid overemphasizing long-term goals over immediate gains.<br/>
Reduces variance: The decay helps to smooth out the noise associated with individual rewards, leading to more consistent and reliable value estimates. <br/>
Adaptive behavior: The ability to control the decay rate (Î») allows the algorithm to adapt its behavior to different tasks. For short-term tasks, a lower Î» is preferred, while long-term tasks benefit from a higher Î»
<br/>
**TD-Gammon** <br/>
Combination of the TD(Î») algorithm and nonlinear function approximation using a multilayer artificial neural network trained by backpropagating TD errors <br/>
<br/>
**Q-Learning with VFA** <br/>
-Minimize MSE loss by stochastic gradient descent <br/>
-Converges to the optimal Q(s, a) using table lookup representation <br/>
-Q-learning with VFA can diverge: Correlations between samples, Non-stationary targets <br/>
**Deep Q-learning DQN)** <br/>
Tries to solve this issues by Experience replay and Fixed Q-targets <br/>
Experience replay <br/>
Stores past experiences in a replay buffer and samples them randomly for training. This reduces correlations between samples and improves  generalization. <br/>
Fixed Q-targets <br/>
Fix the target weights used in the target calculation for multiple updates. This will help to improve stability <br/>
Uses a separate Q-network to generate target Q-values. This network updates its parameters periodically, making the target values less prone to rapid changes. <br/>
**Double Q-learning** <br/>
It utilizes two Q-networks to estimate the Q-values and combines them to form a more robust target for updating both networks.<br/>
Overall, double Q-learning is a valuable technique for mitigating overestimation issues in Q-learning and improving the overall performance of reinforcement learning algorithms. <br/>
ê·¸ë˜ì„œ 2ê°œì˜ estimatorë¥¼ ì´ìš©í•´ì„œ ëœë¤ í•˜ê²Œ ê°ìë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì§„í–‰í•  ë•Œ 2ê°œì˜ ì°¨ì´ë¥¼ ì´ìš©í•´ì„œ ìµœëŒ€í•œ ì¡ìŒì„ ë¹¼ë²„ë¦¬ê³  ì—…ë°ì´íŠ¸í•˜ëŠ” ì „ëµì„ ì·¨í•˜ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.<br/>

# Week 12
Prioritized Experience Replay <br/>
ê¸°ì¡´ì˜ DQNì—ì„œ Experience Replayë¥¼ í†µí•´ Replay bufferì— ì €ì¥ëœ ê³¼ê±°ì˜ ê²½í—˜ì„ ê¸°ì–µí•´ì„œ í•™ìŠµì— ì¬ì‚¬ìš©. ê·¸ëŸ°ë° randomí•˜ê²Œ ì¶”ì¶œí•œ ë°©ë²•ì´ì—ˆìŒ. ì¤‘ìš”í•œ ê²½í—˜ì„ ìì£¼ ì¬ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì„ íƒì ìœ¼ë¡œ ì¶”ì¶œí•˜ê³ ì í•œë‹¤. <br/>
General ë°©ì‹: <br/>
Transitionì˜ magnitude of DQN errorë¥¼ í†µí•´ ê° transitionì˜ ì¤‘ìš”í•œ ì •ë„ë¥¼ í‰ê°€(Proportional to DQN error) <br/>
Stochasticí•œ ë°©ì‹ìœ¼ë¡œ ì ‘ê·¼í•¨ <br/>
<br/>
Policy-based Reinforcement Learning <br/>
-No value function, Learned Policy <br/>
-Deterministic Policy: <br/>
Maps state directly to an action <br/>
-Optimal Policy: <br/>
Policy that maximizes the expected long term reward for the agent <br/>
-When system is not Markov, deterministic policy is easily exploited by the adversary, uniform random policy can be optimal <br/>
Advantages <br/>
-Can learn stochastic policies <br/>
-Better convergence <br/>
-Effective in high dimensional or continuous action spaces <br/>
Disadvantages <br/>
Converge to local optimum <br/>
Inefficient and high variance <br/>
<br/>
Policy Objective Functions <br/>
Find the best ğœƒ in a given policy ğœ‹ğœƒ (ğ‘ |ğ‘,ğœƒ) with parameter ğœƒ <br/>
We measure quality by first estimating policy value at start state ğ‘‰(ğ‘ 0, ğœƒ) and find the policy parameters that maximize ğ‘½(ğ’”ğŸ, ğœ½) <br/>
<br/>
Policy Gradient <br/>
Let the policy parameter based on the gradient of some scalar performance measure ğ½ ğœƒ <br/>
<br/>
Differentiable Policy classes <br/>
While a variety of policy classes exist, differentiable policy classes hold a special significance due to their ability to directly calculate gradients with respect to the policy parameters. This enables efficient and effective policy optimization through gradient-based algorithms.<br/>
Softmax <br/>
:Weight actions using linear combination of features <br/>
-Proportional to exponential weight <br/>
Gaussian Distribution <br/>
:Adequate for continuous action space <br/>
Neural networks <br/>
Can represent complex non-linear relationships between states and actions <br/>
<br/>
Value of a Parameterized Policy<br/>
The value of a parameterized policy in reinforcement learning represents the expected long-term reward the policy will generate starting from a specific state.<br/>
<br/>
Likelihood Ratio Policies
The goal is to find the policy parameter ğœƒ that maximizes the expected reward ğ‘‰ ğœƒ. <br/>
The expected reward can be expressed as a sum over ğ‘ƒ ğœ; ğœƒ and ğ‘… ğœ for all possible trajectories. <br/>
To find the optimal policy, we take the gradient of the expected reward with respect to the policy parameter ğœƒ. <br/>
<br/>
Policy Gradient: Temporal Structure <br/>
Temporal structure : The way information about past rewards and events is incorporated into the learning process for improving the policy. This structure helps the agent understand how its actions over time contribute to the overall outcome and allows it to learn more efficiently and effectively.<br/>
By incorporating information from multiple rewards and taking advantage of the connection between value function and policy gradients, the agent can learn more efficiently and adapt its behavior to achieve long-term goals. However, it's important to consider the computational cost, discounting effects, and need for baseline subtraction when employing temporal structure in practical applications.<br/>
Likelihood Ratio Policies <br/>
-Empirical estimate for m sample trajectories under ğœ‹ <br/>
-Unbiased, but noisy <br/>
(Expected value converges to the true gradient across a large number of samples)
-Solution <br/>
Temporal Structure <br/>
Leveraging the temporal structure of the problem allows for incorporating information from past rewards and experiences into the LRP gradient. This provides a more comprehensive view of the policy's performance and can help reduce noise by taking longer-term effects into account <br/>
Baseline <br/>
Subtracting a baseline value from the LRP gradient can help reduce variance <br/>
Alternative to MC returns<br/>
In certain situations, LRPs can present a computationally efficient alternative to Monte Carlo returns for estimating policy gradients. This is particularly beneficial for problems with large state spaces or complex policies, where traditional Monte Carlo methods may become impractical.<br/>







Reference: <br/>
https://ropiens.tistory.com/86
https://daeson.tistory.com/334 [ëŒ€ì†Œë‹ˆ:í‹°ìŠ¤í† ë¦¬]
