---
layout: post
title:  "Reinforcement Learning"
---

# Week 12
Prioritized Experience Replay <br/>
ê¸°ì¡´ì˜ DQNì—ì„œ Experience Replayë¥¼ í†µí•´ Replay bufferì— ì €ì¥ëœ ê³¼ê±°ì˜ ê²½í—˜ì„ ê¸°ì–µí•´ì„œ í•™ìŠµì— ì¬ì‚¬ìš©. ê·¸ëŸ°ë° randomí•˜ê²Œ ì¶”ì¶œí•œ ë°©ë²•ì´ì—ˆìŒ. ì¤‘ìš”í•œ ê²½í—˜ì„ ìì£¼ ì¬ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì„ íƒì ìœ¼ë¡œ ì¶”ì¶œí•˜ê³ ì í•œë‹¤. <br/>
General ë°©ì‹: <br/>
Transitionì˜ magnitude of DQN errorë¥¼ í†µí•´ ê° transitionì˜ ì¤‘ìš”í•œ ì •ë„ë¥¼ í‰ê°€(Proportional to DQN error) <br/>
Stochasticí•œ ë°©ì‹ìœ¼ë¡œ ì ‘ê·¼í•¨ <br/>
<br/>
Policy-based Reinforcement Learning <br/>
-No value function, Learned Policy <br/>
-Deterministic Policy: <br/>
Maps state directly to an action <br/>
-Optimal Policy: <br/>
Policy that maximizes the expected long term reward for the agent <br/>
-When system is not Markov, deterministic policy is easily exploited by the adversary, uniform random policy can be optimal <br/>
Advantages <br/>
-Can learn stochastic policies <br/>
-Better convergence <br/>
-Effective in high dimensional or continuous action spaces <br/>
Disadvantages <br/>
Converge to local optimum <br/>
Inefficient and high variance <br/>
<br/>
Policy Objective Functions <br/>
Find the best ğœƒ in a given policy ğœ‹ğœƒ (ğ‘ |ğ‘,ğœƒ) with parameter ğœƒ <br/>
We measure quality by first estimating policy value at start state ğ‘‰(ğ‘ 0, ğœƒ) and find the policy parameters that maximize ğ‘½(ğ’”ğŸ, ğœ½) <br/>
<br/>
Policy Gradient <br/>
Let the policy parameter based on the gradient of some scalar performance measure ğ½ ğœƒ <br/>
<br/>
Differentiable Policy classes <br/>
While a variety of policy classes exist, differentiable policy classes hold a special significance due to their ability to directly calculate gradients with respect to the policy parameters. This enables efficient and effective policy optimization through gradient-based algorithms.<br/>
Softmax <br/>
:Weight actions using linear combination of features <br/>
-Proportional to exponential weight <br/>
Gaussian Distribution <br/>
:Adequate for continuous action space <br/>
Neural networks <br/>
Can represent complex non-linear relationships between states and actions <br/>
<br/>
Value of a Parameterized Policy<br/>
The value of a parameterized policy in reinforcement learning represents the expected long-term reward the policy will generate starting from a specific state.<br/>
<br/>
Likelihood Ratio Policies
The goal is to find the policy parameter ğœƒ that maximizes the expected reward ğ‘‰ ğœƒ. <br/>
The expected reward can be expressed as a sum over ğ‘ƒ ğœ; ğœƒ and ğ‘… ğœ for all possible trajectories. <br/>
To find the optimal policy, we take the gradient of the expected reward with respect to the policy parameter ğœƒ. <br/>
<br/>
Policy Gradient: Temporal Structure <br/>
Temporal structure : The way information about past rewards and events is incorporated into the learning process for improving the policy. This structure helps the agent understand how its actions over time contribute to the overall outcome and allows it to learn more efficiently and effectively.<br/>
By incorporating information from multiple rewards and taking advantage of the connection between value function and policy gradients, the agent can learn more efficiently and adapt its behavior to achieve long-term goals. However, it's important to consider the computational cost, discounting effects, and need for baseline subtraction when employing temporal structure in practical applications.<br/>
Likelihood Ratio Policies <br/>
-Empirical estimate for m sample trajectories under ğœ‹ <br/>
-Unbiased, but noisy <br/>
(Expected value converges to the true gradient across a large number of samples)
-Solution <br/>
Temporal Structure <br/>
Leveraging the temporal structure of the problem allows for incorporating information from past rewards and experiences into the LRP gradient. This provides a more comprehensive view of the policy's performance and can help reduce noise by taking longer-term effects into account <br/>
Baseline <br/>
Subtracting a baseline value from the LRP gradient can help reduce variance <br/>
Alternative to MC returns<br/>
In certain situations, LRPs can present a computationally efficient alternative to Monte Carlo returns for estimating policy gradients. This is particularly beneficial for problems with large state spaces or complex policies, where traditional Monte Carlo methods may become impractical.<br/>







