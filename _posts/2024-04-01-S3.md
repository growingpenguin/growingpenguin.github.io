---
layout: post
title:  "S3"
---

# Create S3
## Create two S3 buckets
(1)S3 Bucket name: inferencefilecsv1 (Same as the old inferencefilecsv)<br/>
The S3 bucket, named 'inferencefilecsv', is designated for storing 'inputfile.csv', which contains critical data for inferencing within our internal AI model focused on sloshing prediction <br/>
This repository acts as a pivotal point in our workflow, feeding the AI model with the necessary information to perform its predictive analysis. <br/>
![S3_1](https://github.com/growingpenguin/growingpenguin.github.io/assets/110277903/6a12592e-f595-4d36-b189-ffb7f3312a9c) <br/>
Click on '버킷 만들기' <br/>
![S3_2](https://github.com/growingpenguin/growingpenguin.github.io/assets/110277903/20aaf6d2-2c44-4ea3-9540-a2e82b65ebb3) <br/>
Name the bucket, which should be unique, not only to me, but across all users <br/>
There's a region a bucket needs to exist in, ap-northeast-2 <br/>
S3 live within only one region, but because it is a global service, you can see all the buckets in one view <br/>
Click on the recommended settings, ACL 비활성화 <br/>
Left the Bucket version management disabled by default <br/>
![S3_4](https://github.com/growingpenguin/growingpenguin.github.io/assets/110277903/df03b615-ade6-476d-88b6-1fb319fb1772) <br/>
Another security setting, asking whether to to block public access, since I want to access the S3 by my exe file, I just freed all public access <br/>
I didn't have any important files after all <br/>
I had to check to the warning that my object might go public <br/>
![S3_6](https://github.com/growingpenguin/growingpenguin.github.io/assets/110277903/fd025d2f-597a-41eb-aa0a-711ce2c84fb6) <br/>
For Default Encryption, left the default settings > Click on '버킷 만들기' <br/>


(2)S3 Bucket name: inferenceresultbucket1(Same as the old inferenceresultbucket) <br/>
The S3 bucket, named 'inferenceresultbucket', serves as the storage location for the output results generated by the Lambda function. These results encompass the sloshing predictions derived from processing 'inputfile.csv' through our specialized AI model dedicated to sloshing prediction. This bucket is a crucial component in our data processing chain, capturing and preserving the insights produced by our predictive analytics. <br/>
