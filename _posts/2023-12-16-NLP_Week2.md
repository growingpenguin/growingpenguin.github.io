---
layout: post
title:  "NLP Transformers"
---
# Transformer <br/>
Transformer <br/>
Transformer Architecture <br/>
-Introduced in "Attention is all you need" <br/>
-New Architecture: 기계번역에 있어서의 품질과 훈련 비용에 있어서 순환 신경망(RNN)을 능가 <br/>
ULMFIT <br/>
-Introduced in Universal Language Model Fine-Tuning for Text Classification <br/>
-다양한 말뭉치로부터 언어 모형을 만든 후 적은 양의 label data로 fine tuning하여 최고 수준의 성능 이끌어냄 <br/>
-이후 GPT, BERT 발전의 촉매가 됨 <br/>
<br/>
GPT <br/>
Introduced in Improving Language Understanding by Generative Pre-Training, 2018 <br/>
BERT <br/>
Introduced in BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding, 2018 <br/>
-Transformers architecture와 Non-supervised learning을 결합하여 언어 모형 학습 <br/>
-대부분의 NLP Benchmark 최고의 성능 상회 <br/>
<br/>
# Encoder Decoder Network 
Transformer 이전: NLP의 Sota: LSTM 등 순환신경망(RNN) architecture <br/>
-각 step에서 상태(state) 정보를 다음 step에 전달 <br/>
-이전 step의 정보를 축적하고 이를 사용해 예측 <br/>
![NLP_Transformers2-1](https://github.com/growingpenguin/growingpenguin.github.io/assets/110277903/30a1de96-f25f-4609-b2b9-6a3d0174403e)
<br/>
RNN: 기계 번역 시스템에서의 중요한 역할 수행<br/>
-Encoding: 인코더의 마지막 은닉 상태 <br/>
-Decoding: 인코딩된 상태로부터 출력 시퀀스 생성 <br/>
![NLP_Transformers2-2](https://github.com/growingpenguin/growingpenguin.github.io/assets/110277903/5feda3a3-c24b-4c2c-939c-7d7c70955a8f) 
<br/>
RNN 약점1: 정보 병목(Information BottleNeck) 현상 <br/>
-인코딩 결과인 마지막 은닉 상태가 전체 입력 시퀀를 갈무리함 <br/>
-필연적인 information loss <br/>




