---
layout: post
title:  "NLP Transformers"
---
# Transformer 
Transformer <br/>
Transformer Architecture <br/>
-Introduced in "Attention is all you need" <br/>
-New Architecture: 기계번역에 있어서의 품질과 훈련 비용에 있어서 순환 신경망(RNN)을 능가 <br/>
ULMFIT <br/>
-Introduced in Universal Language Model Fine-Tuning for Text Classification <br/>
-다양한 말뭉치로부터 언어 모형을 만든 후 적은 양의 label data로 fine tuning하여 최고 수준의 성능 이끌어냄 <br/>
-이후 GPT, BERT 발전의 촉매가 됨 <br/>
<br/>
GPT <br/>
Introduced in Improving Language Understanding by Generative Pre-Training, 2018 <br/>
BERT <br/>
Introduced in BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding, 2018 <br/>
-Transformers architecture와 Non-supervised learning을 결합하여 언어 모형 학습 <br/>
-대부분의 NLP Benchmark 최고의 성능 상회 <br/>
<br/>
# Encoder Decoder Network 
Transformer 이전: NLP의 Sota: LSTM 등 순환신경망(RNN) architecture <br/>
-각 step에서 상태(state) 정보를 다음 step에 전달 <br/>
-이전 step의 정보를 축적하고 이를 사용해 예측 <br/>
![NLP_Transformers2-1](https://github.com/growingpenguin/growingpenguin.github.io/assets/110277903/30a1de96-f25f-4609-b2b9-6a3d0174403e)
<br/>
RNN: 기계 번역 시스템에서의 중요한 역할 수행<br/>
-Encoding: 인코더의 마지막 은닉 상태 <br/>
-Decoding: 인코딩된 상태로부터 출력 시퀀스 생성 <br/>
![NLP_Transformers2-2](https://github.com/growingpenguin/growingpenguin.github.io/assets/110277903/5feda3a3-c24b-4c2c-939c-7d7c70955a8f) 
<br/>
RNN 약점1: 정보 병목(Information BottleNeck) 현상 <br/>
-인코딩 결과인 마지막 은닉 상태가 전체 입력 시퀀를 갈무리함 <br/>
-필연적인 information loss <br/>

# Attention Mechanism
Encoder의 각 step의 은닉 상태를 디코더에 모두 활용 <br/>
Attention: <br/>
Encoder의 은닉 상태 출력에 가중치(attention)을 할당 <br/>
![NLP_Transformers2-3](https://github.com/growingpenguin/growingpenguin.github.io/assets/110277903/235e999d-7100-4db3-843f-866a98d97a84)
<br/>
기계번역에서의 Attention <br/>
-원문장과 번역 문장 간의 들어간 align 학습 <br/>
-ex.영어-프랑스 번역 모델 간의 attention 시각화(밝을 수록 강한 Attention) <br/>
RNN 약점2: 순차적인 연산<br/>
-Sequence 전체에 대한 병렬화에 약함 <br/>
![NLP_Transformers2-4](https://github.com/growingpenguin/growingpenguin.github.io/assets/110277903/8333e32c-c30a-4c22-a194-2eb3d8313fd7)
<br/>
Self-Attention(셀프 어텐션): 순환 구조 제거 <br/>
-신경망의 같은 층의 모든 상태에 대한 Attention 작동 <br/>
FFNN(Feed Forward Neural Network) <br/>
-병렬성 확보: 자연어처리 분야의 큰 혁명 <br/>
![NLP_Transformers2-5](https://github.com/growingpenguin/growingpenguin.github.io/assets/110277903/ce88d306-1607-410b-9425-079669711ffa)
<br/>
# NLP의 Transfer Learning
Computer Vision에서의 Transfer Learning <br/>
ResNet 등의 CNN 모델을 하나의 작업에 대해 훈련 이후 새로운 작업에 대해 Fine tuning <br/>
Model: <br/>
-Body(바디):풍부한 원래 Domain에서 다양한 특성 학습 <br/>
(새로운 Domain의 모델 초기값으로 활용) <br/>
-Head(헤드): 새로운 도메인에서 집중적으로 학습<br/>
<br/>
Supervised Learning(지도 학습) <br/>
-Domain별로 학습 <br/>
Transfer Learning(전이 학습) <br/>
-정보가 풍부한 Domain에서 학습된 Model <br/>
-새 Domain 학습 모델의 Body 초기화 <br/>
-Head는 적극 Tuning <br/>
-Body는 약하게 Tuning <br/>
<br/>
Ex. ImageNet <br/>
-수백 만개의 Image에 대해 pre-training: 이미지의 공통된 특징을 Modeling <br/>
-특정 계열의 꽃 품종 분류: 도메인 특화 Fine Tuning <br/>
장점: <br/>
주로 동일한 양의 Label Data의 지도 학습보다 정확도가 높음 <br/>
![NLP_Transformers2-6](https://github.com/growingpenguin/growingpenguin.github.io/assets/110277903/19360d36-db17-47c9-9c19-5ba1b0b0f4c9)
<br/>
NLP에서의 Transfer Learning <br/>
-ULMFit (2018): NLP 분야의 Transfer Learning Idea 마련 <br/>
-Pre-training <br/>
Language Modeling(언어 모델링): 이전 단어를 바탕으로 다음 단어 예측 (Wikipedia 등 접근 가능한 Test set 이용) <br/>
Domain adaptation(도메인 적응): Domain Corpus가 일정 규모 이상 풍부하다면 해당 Corpus에 대해 language modeling <br/>
Fine Tuning(파인 튜닝): 분류 문제라면, Classification Head를 별도로 두어 파인 튜닝  <br/>
(Ex: Movie Review Emotion Classification) <br/>






