---
layout: post
title:  "NLP Transformers"
---
# Transformer 
Transformer <br/>
Transformer Architecture <br/>
-Introduced in "Attention is all you need" <br/>
-New Architecture: 기계번역에 있어서의 품질과 훈련 비용에 있어서 순환 신경망(RNN)을 능가 <br/>
ULMFIT <br/>
-Introduced in Universal Language Model Fine-Tuning for Text Classification <br/>
-다양한 말뭉치로부터 언어 모형을 만든 후 적은 양의 label data로 fine tuning하여 최고 수준의 성능 이끌어냄 <br/>
-이후 GPT, BERT 발전의 촉매가 됨 <br/>
<br/>
GPT <br/>
Introduced in Improving Language Understanding by Generative Pre-Training, 2018 <br/>
BERT <br/>
Introduced in BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding, 2018 <br/>
-Transformers architecture와 Non-supervised learning을 결합하여 언어 모형 학습 <br/>
-대부분의 NLP Benchmark 최고의 성능 상회 <br/>
<br/>
# Encoder Decoder Network 
Transformer 이전: NLP의 Sota: LSTM 등 순환신경망(RNN) architecture <br/>
-각 step에서 상태(state) 정보를 다음 step에 전달 <br/>
-이전 step의 정보를 축적하고 이를 사용해 예측 <br/>
![NLP_Transformers2-1](https://github.com/growingpenguin/growingpenguin.github.io/assets/110277903/30a1de96-f25f-4609-b2b9-6a3d0174403e)
<br/>
RNN: 기계 번역 시스템에서의 중요한 역할 수행<br/>
-Encoding: 인코더의 마지막 은닉 상태 <br/>
-Decoding: 인코딩된 상태로부터 출력 시퀀스 생성 <br/>
![NLP_Transformers2-2](https://github.com/growingpenguin/growingpenguin.github.io/assets/110277903/5feda3a3-c24b-4c2c-939c-7d7c70955a8f) 
<br/>
RNN 약점1: 정보 병목(Information BottleNeck) 현상 <br/>
-인코딩 결과인 마지막 은닉 상태가 전체 입력 시퀀를 갈무리함 <br/>
-필연적인 information loss <br/>

# Attention Mechanism
Encoder의 각 step의 은닉 상태를 디코더에 모두 활용 <br/>
Attention: <br/>
Encoder의 은닉 상태 출력에 가중치(attention)을 할당 <br/>
![NLP_Transformers2-3](https://github.com/growingpenguin/growingpenguin.github.io/assets/110277903/235e999d-7100-4db3-843f-866a98d97a84)
<br/>
기계번역에서의 Attention <br/>
-원문장과 번역 문장 간의 들어간 align 학습 <br/>
-ex.영어-프랑스 번역 모델 간의 attention 시각화(밝을 수록 강한 Attention) <br/>
RNN 약점2: 순차적인 연산<br/>
-Sequence 전체에 대한 병렬화에 약함 <br/>
![NLP_Transformers2-4](https://github.com/growingpenguin/growingpenguin.github.io/assets/110277903/8333e32c-c30a-4c22-a194-2eb3d8313fd7)
<br/>
Self-Attention(셀프 어텐션): 순환 구조 제거 <br/>
-신경망의 같은 층의 모든 상태에 대한 Attention 작동 <br/>
FFNN(Feed Forward Neural Network) <br/>
-병렬성 확보: 자연어처리 분야의 큰 혁명 <br/>




